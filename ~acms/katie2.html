<html>
<body bgcolor = #f8ffff >

<h3>
Multiscale Signatures of Beatles Songs in a Cover Song Task</h3>
<p></p>
<i> <p>Katie Kinnard</p> </i>
<p style="font-size:smaller">Dartmouth College</p>
<p><img src="https://www.math.dartmouth.edu/~acms/line.gif">
<br>

<p>
<table border="0" width="530" cellspacing="0">

<td>
 While there are several Music Information Retrieval (MIR) classification and comparison tasks, this work focuses on the cover song task. Given a particular piece of music, the goal of the cover song task is to find all the different recordings of the given song by a variety of artists in a set of music. For this work, our dataset is a collection of songs by the Beatles. Our approach begins by building a novel multiscale signature for each song that captures repetitive structure at several scales, while also being of a manageable dimension. We then apply a metric to this representation space of song signatures, allowing for fine-tuned comparison between songs. This multiscale approach differs from those in the literature that largely consider single-scale, single-feature representations.</td>

</table>

<p><img src="https://www.math.dartmouth.edu/~acms/line.gif"></p>

<p style="font-size:smaller">Back to <a href="index.html">ACMS schedule</a></p>


</body>
</html>


