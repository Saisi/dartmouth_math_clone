<!-- -*- html -*- -->
<HTML>
<HEAD>
<meta http-equiv="Content-Type" content="text/html; charset=windows-1252">
<TITLE>Real-world Statistics Submissions: Math 50, WINTER 2006</TITLE>
</HEAD>

<BODY BGCOLOR="#FFFFFF">

<!-- Enter some local variables -->

<!-- WARNING: Don't change the following lines. -->
<FORM action="" method="post">
<INPUT type="hidden" name="command" value="post">
<!-- WARNING: Don't change the previous lines. -->




<H2>Real-world Statistics Submissions: Math 50, WINTER 2006</H2>


<p>Each week you should dig up a
statistical example from the media, web, or other real-world source,
write a paragraph which you post below,
and be ready to explain it in class (I will pick on you, randomly of course!).
If it relates to the week's content, all the better.
As the course progresses we will be able to connect these to the material. 
<i>Why do it?</i>
<ol>
<li>statistics is all around us, affects policies, our lives, etc.
The discipline was <i>invented</i> to deal with these questions.
<li>communication skills
<li>project ideas for you and all of us
<li>choose topics that interest you
</ol>





<p>
Previously posted submissions are 
<A HREF= "index.html#comments_start_here">below</A> the
input form.
</p>

<p>
To submit a comment, you <b>must</b> provide your name.
If you don't provide your name and submit the comment,
the comment will <b>disappear</b> from the text field
and you will have to <b>type it again</b>.
</p>

<p>
You may embed HTML in this page such as &lt;BR&gt;, &lt;P&gt;,
<BR>
&lt;B&gt;...&lt;/B&gt;, etc for formatting purposes.
I encourage you to include <b>links</b> like this: &lt;a href="http://interesting.site.org/"&gt;check it out!&lt;/a&gt;
<BR>
You may also cut from a document and paste into the comments area.
</p>

<TABLE cellspacing="0" cellpadding="2" border="0" WIDTH=550 BGCOLOR="#c9dce1">
<TR>
<TH>Name:</TH>
<TD>
  <TABLE>
    <TR>
    <TD><INPUT type="text" name="name"
                           size="60" maxsize="60"></TD>
    </TR>
  </TABLE>
</TD>
</TR>

<!--
<TR>
<TH>News Source:</TH>
<TD>
  <TABLE>
    <TR>
    <TD><INPUT type="text" name="newsSource"
                           size="60" maxsize="60"></TD>
    </TR>
  </TABLE>
</TD>
</TR>
-->

<TR>
<TH>Comment:</TH>
<TD><TEXTAREA name="comment" cols="62" rows="10" wrap="auto">
</TEXTAREA></TD>
</TR>
<TR>
<TH>&nbsp;</TH>
<TD><INPUT type="submit" value="Post Your Comment">
<INPUT type="reset" value="Reset This Form"></TD>
</TR>
</TABLE>

</FORM>

<P>
<A NAME="comments_start_here"><P>
<!-- WARNING: Don't mess with the stuff below. -->

<HR noshade>

<TABLE cellspacing="2" cellpadding = 5 border="0" 
BGCOLOR = "#c9dce1" WIDTH = 550>
<TR >
<TH WIDTH=75 ALIGN=left>Name:</TH>
<TD WIDTH=475 ALIGN=left>Chetan Mehta</TD>
</TR>
<TR>
<TH ALIGN=left>Date:</TH>
<TD ALIGN=left>March 11, 2006 (11:22)</TD>
</TR>
<!--
<TR>
<TH ALIGN=left>News Source:</TH>
<TD ALIGN=left></TD>
</TR>
-->
<TR>
<TH ALIGN=left>Comment:</TH>
<TD ALIGN=left>Very cool article on markets and the accuracy of predictions. I think they mention the book's example for Bayesian inference. It's on the NYT's paid service, so I've copied and pasted:

March 11, 2006
The Future Divined by the Crowd
By JOE NOCERA

LAST Sunday was Oscar night, which of course meant movie stars, glamour, Jon Stewart's jokes and — drum roll, please! — Michael J. Mauboussin's annual economics experiment. Or rather, his annual economics eye-opener.

Mr. Mauboussin is a well-known Wall Street strategist, now with Legg Mason, who gained a following for an offbeat, scholarly newsletter called The Consilient Observer, in which he tackled such diverse subjects as what investors can learn from Tupperware parties and why stress can lower investment performance. His work is sometimes dense, but always original.

Since 1993, Mr. Mauboussin has also been an adjunct professor at Columbia Business School. Every year, as the Academy Awards approach, he asks his students to vote for the winners in 12 categories, not just the biggies but some relatively obscure ones, like best film editing and best art direction. Then, after the Oscars have been awarded, he tallies the results and compares the students' predictions with the winners.

This year, the pick that got the most votes — the consensus pick, he calls it — turned out to be right in 9 of the 12 categories, including, amazingly enough, film editing and art direction. And yet, of the 47 students who participated, only one matched the accuracy of the consensus. None did better, and most did much worse; according to Mr. Mauboussin, the average number of correct answers per ballot this year was only 4.1. "It has never failed," he said. "The consensus invariably does much better than the average student."

The point of Mr. Mauboussin's little experiment is to illustrate the power of so-called prediction markets, in which groups of people guess or bet on something, with the results aggregated into a consensus. Prediction markets, while not perfect, are surprisingly accurate — certainly more accurate than individual experts or polls, research has found.

The granddaddy of prediction markets, the Iowa Electronic Market, which the University of Iowa has run since the late 1980's, allows people to make election predictions. The consensus almost always beats the polling data; in the last presidential election, for instance, it not only steadfastly predicted a Bush victory, but came within 1.1 percentage point of the actual result.

PREDICTION markets aren't just for curious academics anymore; in the last few years, there has been an explosion of interest. A handful of dot-coms, with names like Hollywood Stock Exchange, hedgestreet.com and Newsfutures, offer people the chance to predict everything from how much a new movie will gross in its first month to whether gasoline prices will rise. (Hedgestreet.com's slogan is: "It's your economy. Trade it.")

Some sites plan to permit Wall Street traders to begin trading on such questions, much in the way commodities traders bet on the future price of soybeans. Perhaps most intriguing, companies have begun experimenting with prediction markets as a new kind of forecasting tool. Though it's still a little early to say for sure, prediction markets could wind up changing the ways companies go about making decisions.

There are two recent events that gave rise to this new interest. The first was a quasi-disaster. In 2003, John M. Poindexter, who was heading up a small research group inside the Pentagon, tried to create a public prediction market for geopolitical events, including possible terrorist attacks. But the criticism was swift — Mr. Poindexter was accused of creating "terrorism futures" — and the program was killed. Still, the publicity got a lot of businesspeople thinking about the utility of prediction markets. "The Pentagon," chuckled Alexander Costakis, who runs the Hollywood Stock Exchange, "put it on the map."

The second event was the publication, in 2004, of "The Wisdom of Crowds," by James Surowiecki, a business columnist with The New Yorker. Mr. Surowiecki's title was purposely meant to echo — and contradict — Charles Mackay's famous 1841 work, "Extraordinary Popular Delusions and the Madness of Crowds," which Mr. Surowiecki describes, correctly, as "an endlessly entertaining chronicle of mass manias and collective follies." No one argues that crowds don't sometimes go mad; from Dutch tulips to dot-com bubbles, the evidence is pretty irrefutable. But Mr. Surowiecki set out to show that far more often, the crowd got it right.

His book is filled with examples of the power of "collective intelligence," as he likes to call it. For instance, Mr. Surowiecki describes a case in which a naval officer located a sunken submarine by asking people with bits of specific expertise to take their best guess. Their collective answer turned out to be within a few hundred yards of the submarine's location.

Another example came on Jan. 28, 1986, the day the Challenger shuttle exploded. All the stocks of rocket makers were down that day, of course, but by far the hardest hit was Morton Thiokol — which made the faulty part that was responsible for the disaster. The stock market is nothing if not a prediction market writ large. But how could the market possibly have known about Morton Thiokol's culpability on the first day? No information had been revealed. There was no excessive insider selling. There wasn't even any media speculation as to the culprit. Yet, somehow the market knew.

Two of the people who read Mr. Surowiecki's book were Eric E. Schmidt, the chief executive of Google, and Bo Cowgill, a young Google executive. Being a forward-thinking geek, Mr. Cowgill decided to set up some prediction markets at Google, and management gave him the go-ahead. He's been doing it for a year now.

The Google market is internal only — the voters are all Google employees. To avoid insider-trading problems, Mr. Cowgill stays away from any stock price or quarterly earnings questions. And to entice people to participate he gives away T-shirts. And he asks pretty useful questions: "When will a product launch? How much will a particular feature be used? How many full-time people will accept jobs at Google in the next quarter?" he said.

The utility of such forecasting is clear: it is important for companies to know when a product will be introduced, and tapping into the collective intelligence of employees to predict the date is at least as good a way to find out as talking to the project manager. In fact, it is probably a better way; at a recent conference, Mr. Cowgill presented a series of slides showing that Google's internal market was amazingly accurate, especially in markets in which lots of people participated.

Google is hardly the only company doing this. Eli Lilly has used internal markets to forecast drug development. Hewlett-Packard has become a fan. Emile Servan-Schreiber, the chief executive of Newsfutures, sells software to companies that helps them set up prediction markets. His first customer, in 2003, was Eli Lilly. Now, he says, he is working with 12 to 15 companies.

Yahoo has gone about it a different way, setting up a public market called Tech Buzz. It allows people to make bets on which technologies will be popular in the future; the point, for Yahoo, is to get out in front of popular coming search terms. "The more we know about trends, the better we can adapt our search services," said David Pennock, who runs the predictive markets for Yahoo.

Still, it's hard to know for sure whether predictive markets will turn out to be the next big thing. "There is a lot of interest, but we're still missing the big success story," said Robin D. Hanson, a professor at George Mason University and a reigning expert in the field. "People are trying it on small, cute things. We haven't seen a company that's made $1 million using this stuff."

Indeed, Mr. Cowgill told me that he really had no idea how Google's management used the forecasts generated by the company's prediction markets. Mr. Servan-Schreiber said the companies that were showing interest were "early adopters." One of his clients, Corning, has set up a prediction market to gauge demand for L.C.D. screens, which it manufactures. "Corning has billions on the line, and it needs to be able to assess demand," Mr. Servan-Schreiber said. Maybe that experiment will turn out to be the success story Mr. Hanson is looking for. (Corning, alas, declined to comment.)

The real problem for prediction markets is that the notion is so counterintuitive. We're used to the idea that experts, full of specialized knowledge, will get it right far more often than a crowd of people with no special insight. The thought that the reverse is actually closer to the truth is hard to get one's head around.

"I'm not sure why it works," Mr. Mauboussin said. But then he offered a theory. "All of us walk around with a little information and a substantial error term. And when we aggregate our results, the errors tend to cancel each other out and what is distilled is pure information."

Well, maybe. Myself, I'm going with the "Shakespeare in Love" theory. You recall, surely, the producer played by Geoffrey Rush. Whenever he was asked how it was that a play that seemed to be such a hopeless muddle during rehearsal was transformed into a gem on opening night, he had a stock reply, one that seems to describe prediction markets perfectly:

"It's a miracle."
</TD>
</TR>
</TABLE>
<P>

<HR noshade>

<TABLE cellspacing="2" cellpadding = 5 border="0" 
BGCOLOR = "#c9dce1" WIDTH = 550>
<TR >
<TH WIDTH=75 ALIGN=left>Name:</TH>
<TD WIDTH=475 ALIGN=left>Brian and Tom</TD>
</TR>
<TR>
<TH ALIGN=left>Date:</TH>
<TD ALIGN=left>February 27, 2006 (07:50)</TD>
</TR>
<!--
<TR>
<TH ALIGN=left>News Source:</TH>
<TD ALIGN=left></TD>
</TR>
-->
<TR>
<TH ALIGN=left>Comment:</TH>
<TD ALIGN=left>This is simply some additional information about our project, more specifically the survey that we will be giving out:

1. Gender:

2. Class Year: 

3. Are you a member of a coed, fraternity, or sorority (yes/no)?

4. Do you drink?

5. On average, how many games of pong would you say you play in a two-week period?

6. What percentage of your pong games do you think you win?

As Tom already mentioned, we are most interested in seeing how the reported winning percentage differs from the true mean of 50%, as this will give us an idea of how much Dartmouth students tend to inflate their drinking abilities and pong prowess.
</TD>
</TR>
</TABLE>
<P>

<HR noshade>

<TABLE cellspacing="2" cellpadding = 5 border="0" 
BGCOLOR = "#c9dce1" WIDTH = 550>
<TR >
<TH WIDTH=75 ALIGN=left>Name:</TH>
<TD WIDTH=475 ALIGN=left>Alison</TD>
</TR>
<TR>
<TH ALIGN=left>Date:</TH>
<TD ALIGN=left>February 26, 2006 (22:13)</TD>
</TR>
<!--
<TR>
<TH ALIGN=left>News Source:</TH>
<TD ALIGN=left></TD>
</TR>
-->
<TR>
<TH ALIGN=left>Comment:</TH>
<TD ALIGN=left>Lighthouse Problem Revisited!! I'm studying the lighthouse problem with two parameters (distance from lighthouse to shore in addition to location of the lighthouse along the shore). It's fun- you will all hear about it Wednesday. </TD>
</TR>
</TABLE>
<P>

<HR noshade>

<TABLE cellspacing="2" cellpadding = 5 border="0" 
BGCOLOR = "#c9dce1" WIDTH = 550>
<TR >
<TH WIDTH=75 ALIGN=left>Name:</TH>
<TD WIDTH=475 ALIGN=left>Chetan</TD>
</TR>
<TR>
<TH ALIGN=left>Date:</TH>
<TD ALIGN=left>February 26, 2006 (21:39)</TD>
</TR>
<!--
<TR>
<TH ALIGN=left>News Source:</TH>
<TD ALIGN=left></TD>
</TR>
-->
<TR>
<TH ALIGN=left>Comment:</TH>
<TD ALIGN=left>I will be working with Seth on the attractiveness project.</TD>
</TR>
</TABLE>
<P>

<HR noshade>

<TABLE cellspacing="2" cellpadding = 5 border="0" 
BGCOLOR = "#c9dce1" WIDTH = 550>
<TR >
<TH WIDTH=75 ALIGN=left>Name:</TH>
<TD WIDTH=475 ALIGN=left>Dave</TD>
</TR>
<TR>
<TH ALIGN=left>Date:</TH>
<TD ALIGN=left>February 26, 2006 (21:22)</TD>
</TR>
<!--
<TR>
<TH ALIGN=left>News Source:</TH>
<TD ALIGN=left></TD>
</TR>
-->
<TR>
<TH ALIGN=left>Comment:</TH>
<TD ALIGN=left>I plan to sit at several places on campus (novack stairs, food court, etc.) and log the time of every person entering an area to find if the distribution of waiting times follows a gamma distribution (or if not, what distribution it does follow). Possible problems I foresee are varying rates by time of day, and within time periods (which I will attempt to counter by choosing similar time periods which include no events such as class beginnings to taint the data), and groups of people (I plan not count anyone who's obviously together, but this can be tricky.).</TD>
</TR>
</TABLE>
<P>

<HR noshade>

<TABLE cellspacing="2" cellpadding = 5 border="0" 
BGCOLOR = "#c9dce1" WIDTH = 550>
<TR >
<TH WIDTH=75 ALIGN=left>Name:</TH>
<TD WIDTH=475 ALIGN=left>Seth</TD>
</TR>
<TR>
<TH ALIGN=left>Date:</TH>
<TD ALIGN=left>February 26, 2006 (18:53)</TD>
</TR>
<!--
<TR>
<TH ALIGN=left>News Source:</TH>
<TD ALIGN=left></TD>
</TR>
-->
<TR>
<TH ALIGN=left>Comment:</TH>
<TD ALIGN=left>I'm planning to take a look at how perceptions of attractiveness differ between men and women by constructing estimates of the underlying pdfs used to assess good-lookingness. To put it more simply, if you see a person, what are the chances you'll place them in a certain attractivness range, given their gender? I'm going to assemble estimates of these pdfs using data from <www.hotornot.com>, a website where people can rate each other's attractiveness on a scale of 1-10 (I encourage you to check it out; it's kind of creepy). Using histograms, what we've learned of probability distributions, and ML techniques, I hope to answer questions like: are men and women "assessed" according to similar or different pdfs? If so, what do the characteristics (e.g., mean, variance, potential bimodality) of these underlying distributions say about the standards people use to judge each other? </TD>
</TR>
</TABLE>
<P>

<HR noshade>

<TABLE cellspacing="2" cellpadding = 5 border="0" 
BGCOLOR = "#c9dce1" WIDTH = 550>
<TR >
<TH WIDTH=75 ALIGN=left>Name:</TH>
<TD WIDTH=475 ALIGN=left>mark, kasia, matt</TD>
</TR>
<TR>
<TH ALIGN=left>Date:</TH>
<TD ALIGN=left>February 26, 2006 (12:50)</TD>
</TR>
<!--
<TR>
<TH ALIGN=left>News Source:</TH>
<TD ALIGN=left></TD>
</TR>
-->
<TR>
<TH ALIGN=left>Comment:</TH>
<TD ALIGN=left>we are planning to analyze the accuracy of weather predictions.  We will look at 10-day forcasts for the capitols of all 50 states over a ten day period and analyze the relationship between accuracy and length of future prediction.  We plan to analyze multiple moments of the temperature distributions and will employ both matlab and stata.  at this point  we predict our project will be hazy with a  60% chance of success.</TD>
</TR>
</TABLE>
<P>

<HR noshade>

<TABLE cellspacing="2" cellpadding = 5 border="0" 
BGCOLOR = "#c9dce1" WIDTH = 550>
<TR >
<TH WIDTH=75 ALIGN=left>Name:</TH>
<TD WIDTH=475 ALIGN=left>Brian and Tom</TD>
</TR>
<TR>
<TH ALIGN=left>Date:</TH>
<TD ALIGN=left>February 26, 2006 (11:32)</TD>
</TR>
<!--
<TR>
<TH ALIGN=left>News Source:</TH>
<TD ALIGN=left></TD>
</TR>
-->
<TR>
<TH ALIGN=left>Comment:</TH>
<TD ALIGN=left>We plan to study drinking at Dartmouth to see how students will lie about their drinking habits. We have designed a survey in the hopes of recieving respones in regards to students drinking habits and pong playing. Our primary interest is looking at the proportion of pong games won that students claim to play, and see how the average number of games would correspond to what the true median must be: 50%. A difference to either end of the spectrum (though more likely on the higher side), would help prove our belief that students will be highly inclined to lie about their drinking habits.

We also plan to collect and analyze data regarding the number of greek students on campus, and to see if we can create some sort of equation to model the amount that students drink or play drinking games (possible correlations: year, gender, greek affiliation).</TD>
</TR>
</TABLE>
<P>

<HR noshade>

<TABLE cellspacing="2" cellpadding = 5 border="0" 
BGCOLOR = "#c9dce1" WIDTH = 550>
<TR >
<TH WIDTH=75 ALIGN=left>Name:</TH>
<TD WIDTH=475 ALIGN=left>SK</TD>
</TR>
<TR>
<TH ALIGN=left>Date:</TH>
<TD ALIGN=left>February 25, 2006 (14:20)</TD>
</TR>
<!--
<TR>
<TH ALIGN=left>News Source:</TH>
<TD ALIGN=left></TD>
</TR>
-->
<TR>
<TH ALIGN=left>Comment:</TH>
<TD ALIGN=left>I intend to look at US train derailment statistics for two years: 1990 and 2000. During this time, the Federal Railroad Administration (FRA) reported that 1993-1999 were the safest years for American trains in terms of all kinds of incidents (not just derailments, but collisions, signal problems, etc.). I would like to see whether or not train derailments followed this trend, to what extent each FRA region (there are 8) did, and which track type (9 classes, designated by what speed they can accommodate). I intend to use Poisson distributions to compare for each region and to perform 2-sample tests among the busier rail corridors in the US</TD>
</TR>
</TABLE>
<P>

<HR noshade>

<TABLE cellspacing="2" cellpadding = 5 border="0" 
BGCOLOR = "#c9dce1" WIDTH = 550>
<TR >
<TH WIDTH=75 ALIGN=left>Name:</TH>
<TD WIDTH=475 ALIGN=left>Neha and Pauline</TD>
</TR>
<TR>
<TH ALIGN=left>Date:</TH>
<TD ALIGN=left>February 25, 2006 (13:20)</TD>
</TR>
<!--
<TR>
<TH ALIGN=left>News Source:</TH>
<TD ALIGN=left></TD>
</TR>
-->
<TR>
<TH ALIGN=left>Comment:</TH>
<TD ALIGN=left>Our project will look into the development of IQ testing and the underlying biases in the tests.  The first method used was craniometry, the measuring of the skull, was not able to explain the differences in intelligence because there was not much variation in skull size.  Later developments focused on racial differences to account for intelligence.  To overcome the bias in these studies formal IQ tests were introduced.  While these tests are not perfect they have managed to overcome much of the bias present in the earlier methods.</TD>
</TR>
</TABLE>
<P>

<HR noshade>

<TABLE cellspacing="2" cellpadding = 5 border="0" 
BGCOLOR = "#c9dce1" WIDTH = 550>
<TR >
<TH WIDTH=75 ALIGN=left>Name:</TH>
<TD WIDTH=475 ALIGN=left>Seth Zimmerman</TD>
</TR>
<TR>
<TH ALIGN=left>Date:</TH>
<TD ALIGN=left>February 21, 2006 (11:54)</TD>
</TR>
<!--
<TR>
<TH ALIGN=left>News Source:</TH>
<TD ALIGN=left></TD>
</TR>
-->
<TR>
<TH ALIGN=left>Comment:</TH>
<TD ALIGN=left>http://www.washingtonpost.com/wp-dyn/content/article/2006/02/21/AR2006022100623_2.html

This article discusses a comprehensive study of the correlations between various  behavior patterns and the likelihood of graduating from college. The report examines lots of factors-- ranging from grades to time off from school to high shool class schedule-- and describes how they affect graduation chances. Based on this information, the report's author makes many recommendations. For instance, he warns students considering a gap year to think very carefully about their decision, because many people never make it back to school. He also recommends summer school, counsels against remedial classes, etc, etc. In fact, he weighs so many possible influences that one wonders how many of the observed effects are artifacts of datamining-- i.e., type I errors.  </TD>
</TR>
</TABLE>
<P>

<HR noshade>

<TABLE cellspacing="2" cellpadding = 5 border="0" 
BGCOLOR = "#c9dce1" WIDTH = 550>
<TR >
<TH WIDTH=75 ALIGN=left>Name:</TH>
<TD WIDTH=475 ALIGN=left>Neha </TD>
</TR>
<TR>
<TH ALIGN=left>Date:</TH>
<TD ALIGN=left>February 18, 2006 (11:01)</TD>
</TR>
<!--
<TR>
<TH ALIGN=left>News Source:</TH>
<TD ALIGN=left></TD>
</TR>
-->
<TR>
<TH ALIGN=left>Comment:</TH>
<TD ALIGN=left>A survey done in Lynn, MA showed that 40% of the residents who answered the survey did not know how to call for emergency help.   Surveyors attributed this response to the lack of English proficiency amongst residents and incomplete foreign language translations for the city's telephone directory.  The 1/3 of the participants were seniors or chronically unemployed individuals.  25% of the participants had lived in Lynn for 1 to 5 years.  The survey also found that downtown residents typically earned lass than 15,000 a year.  The response rate of the survey was 1 in 11 which is much better than the 1 in 20 response rate that a large survey would typically get.  
Source:
http://www.thedailyitemoflynn.com/news/view.bg?articleid=11359</TD>
</TR>
</TABLE>
<P>

<HR noshade>

<TABLE cellspacing="2" cellpadding = 5 border="0" 
BGCOLOR = "#c9dce1" WIDTH = 550>
<TR >
<TH WIDTH=75 ALIGN=left>Name:</TH>
<TD WIDTH=475 ALIGN=left>SK</TD>
</TR>
<TR>
<TH ALIGN=left>Date:</TH>
<TD ALIGN=left>February 18, 2006 (01:58)</TD>
</TR>
<!--
<TR>
<TH ALIGN=left>News Source:</TH>
<TD ALIGN=left></TD>
</TR>
-->
<TR>
<TH ALIGN=left>Comment:</TH>
<TD ALIGN=left>http://www.medicalnewstoday.com/medicalnews.php?newsid=37948

Dutch researchers have found that two years of therapy and a drug known as rituximab significantly immproves the chances of survival for people afflicted with a cancer known as lymphoma (info can be found here: http://en.wikipedia.org/wiki/Lymphomas ), extending survival by about 2.5 years. 465 patients were randomly assigned to either various drugs or to this therapy and rituximab. Responding patients were then re-assigned in the same manner to either choices or to just be observed. If the patients got rituximab, they would get doses every three months for two years (this was most likely a long experiment). They found that the patients who got therapy and rituximab were surviving at higher proportions than those who got something else (29% vs 16%, p < 0.0001, far below the alpha level of 0.05) and lived for longer (33 months vs. 20 months, p=0.0003, also far below the alpha level of 0.05). This finding could greatly alter the way we treat lymphoma, for which 20,000 people in Western Europe alone are diagnosed annually and for which 40,000 Western Europeans are currently seeking treatment (one could imagine that with a larger population, more Americans are affected by this illness).</TD>
</TR>
</TABLE>
<P>

<HR noshade>

<TABLE cellspacing="2" cellpadding = 5 border="0" 
BGCOLOR = "#c9dce1" WIDTH = 550>
<TR >
<TH WIDTH=75 ALIGN=left>Name:</TH>
<TD WIDTH=475 ALIGN=left>Tom Healy</TD>
</TR>
<TR>
<TH ALIGN=left>Date:</TH>
<TD ALIGN=left>February 14, 2006 (20:53)</TD>
</TR>
<!--
<TR>
<TH ALIGN=left>News Source:</TH>
<TD ALIGN=left></TD>
</TR>
-->
<TR>
<TH ALIGN=left>Comment:</TH>
<TD ALIGN=left>A relevant article for today:

'Generations X and Y tune into Net to Spark Romance this Valentine's Day'

http://biz.yahoo.com/bw/060207/20060207005308.html?.v=1

This survey was of 1000 young persons aged 18-35. It offers a number of statistics regarding the rates of persons who would use online methods this valenties day, including downloading songs that show their affection and making online dinner reservations. It would be very interesting to see a similarly sized survey for elderly persons to look at the differing rates of online usage, so see how the generation gaps could be affecting these rates.</TD>
</TR>
</TABLE>
<P>

<HR noshade>

<TABLE cellspacing="2" cellpadding = 5 border="0" 
BGCOLOR = "#c9dce1" WIDTH = 550>
<TR >
<TH WIDTH=75 ALIGN=left>Name:</TH>
<TD WIDTH=475 ALIGN=left>seth zimmerman</TD>
</TR>
<TR>
<TH ALIGN=left>Date:</TH>
<TD ALIGN=left>February 14, 2006 (14:25)</TD>
</TR>
<!--
<TR>
<TH ALIGN=left>News Source:</TH>
<TD ALIGN=left></TD>
</TR>
-->
<TR>
<TH ALIGN=left>Comment:</TH>
<TD ALIGN=left>http://www.nytimes.com/2006/02/12/national/12homicide.html

This article about rising homicide rates in major cities brings up a few interesting statistical issues. 

The article mentions numerous examples of cities where the homicide rate has increased exponentially, and argues that this may represent a resurgent trend toward violent crime. Others, however, claim that it is inevitable that some cities will show increases even if national trends are mixed. That is, even if the pdf of homicide trends is heavily weighted towards a decrease, there are enough cities that even a small upper tail will translate into a lot of anecdotal evidence. 

Further complicating matters is the fact that the city itself may not be an appropriate unit-- even within cities, certain groups are almost immune from crime, while others account for disproportionate amounts. 

What we see here is an almost intractable practical problem: what criteria should a trend exhibit for it to be considered a "national" one? And, if we could construct such a statistic, would it mean anything in real term?  
</TD>
</TR>
</TABLE>
<P>

<HR noshade>

<TABLE cellspacing="2" cellpadding = 5 border="0" 
BGCOLOR = "#c9dce1" WIDTH = 550>
<TR >
<TH WIDTH=75 ALIGN=left>Name:</TH>
<TD WIDTH=475 ALIGN=left>Mark Christman</TD>
</TR>
<TR>
<TH ALIGN=left>Date:</TH>
<TD ALIGN=left>February 13, 2006 (13:40)</TD>
</TR>
<!--
<TR>
<TH ALIGN=left>News Source:</TH>
<TD ALIGN=left></TD>
</TR>
-->
<TR>
<TH ALIGN=left>Comment:</TH>
<TD ALIGN=left>http://www.alternet.org/story/31942/

Gay Bowel Syndrome?  Yeah, Right

This is just another example of my sheer hatred for idiots using math.  The article revolves around the Dr. Paul Cameron, an anit-homosexual psychiatrist, who often uses survey results and findings based on hypothesis testing to condemn homosexuality.  Politics aside, this article shows that bad math in the wrong hands can be quite devastating despite the fact that "legitimate scientists have identified and described in detail multiple fundamental flaws in [his source's] methodology and statistical analysis, any one of which would be enough to destroy its credibility," he still rallies a strong following.  This just outlines the importance of modeling, defining, and using appropriate estimators and estimating techniques before jumping to conclusions.  Mathematical missinformation in the wrong hands can be quite damaging.</TD>
</TR>
</TABLE>
<P>

<HR noshade>

<TABLE cellspacing="2" cellpadding = 5 border="0" 
BGCOLOR = "#c9dce1" WIDTH = 550>
<TR >
<TH WIDTH=75 ALIGN=left>Name:</TH>
<TD WIDTH=475 ALIGN=left>Mark Christman</TD>
</TR>
<TR>
<TH ALIGN=left>Date:</TH>
<TD ALIGN=left>February 13, 2006 (13:20)</TD>
</TR>
<!--
<TR>
<TH ALIGN=left>News Source:</TH>
<TD ALIGN=left></TD>
</TR>
-->
<TR>
<TH ALIGN=left>Comment:</TH>
<TD ALIGN=left>Since I missed posting last week, I guess I should do 2 this week...

The first is in reference to one of the math presentaions from a little over a week ago.  The presentation was regarding noise in various electoral processes and what its minimum threshold is.  Before talking about the answer, what was most interesting was the fact that the presenter stumbled upon this mathematical proof while working on an optimization scheme that was completely unrelated to voting theory.  Anyways, the concept of the proof is pretty straight forward (if you take the presenter's assumptions a given).  He envisioned each vote being a random spot a spherical plane.  Then, if each vote has a probability x of flipping to another candidate, it can be represented by an angle extending for the center of the sphere to its surface.  If it happens that one point on the sphere spanned by the angle is for Candidate A and the other is for Candidate B, that vote flips.  He then realized that this can be reduce to 2 dimensions, with a portion of a circle's circumference representing candidate A and rest representing candidate B.  From this, he was able to calculate that the minimun "noise" resulting in this process is (arccos(1-2x))pi^-1.  The major stipulations are that x cannot exceed .5 and that only 2 candidates may be present.  The latter assumption (plus a few more technical ones) indicates that a majority voting system, therefore, yields the least noise.</TD>
</TR>
</TABLE>
<P>

<HR noshade>

<TABLE cellspacing="2" cellpadding = 5 border="0" 
BGCOLOR = "#c9dce1" WIDTH = 550>
<TR >
<TH WIDTH=75 ALIGN=left>Name:</TH>
<TD WIDTH=475 ALIGN=left>Chetan Mehta</TD>
</TR>
<TR>
<TH ALIGN=left>Date:</TH>
<TD ALIGN=left>February 13, 2006 (11:46)</TD>
</TR>
<!--
<TR>
<TH ALIGN=left>News Source:</TH>
<TD ALIGN=left></TD>
</TR>
-->
<TR>
<TH ALIGN=left>Comment:</TH>
<TD ALIGN=left>Want to know how much your home is worth?
Then go right here: http://www.zillow.com/

It tells the estimate for your home as well as several other homes in your neighbourhood. Information from both the assessor's  office and actual home sales prices is being used here. A more interesting question is how you would go about measuring the variation of prices between those paid at an actual sale and those quoted on the website. If all homeowners, upon purchasing a home, were required to enter the amount they paid into a website, this would become a fantastic resource for future buyers. It would also give the government an excellent database for tracking the housing sector and finding regions where there are 'bubbles'. The composite of any such data would  be the best estimator for the actual home price in a region. Any variation in price over time must primarily be attributed to appreciation in the worth of the house and/or investments made by the homeowner (new bathroom, pool, etc.).

*Link was taken from the Freakonomics blog at freakonomics.com/blog</TD>
</TR>
</TABLE>
<P>

<HR noshade>

<TABLE cellspacing="2" cellpadding = 5 border="0" 
BGCOLOR = "#c9dce1" WIDTH = 550>
<TR >
<TH WIDTH=75 ALIGN=left>Name:</TH>
<TD WIDTH=475 ALIGN=left>Chetan Mehta</TD>
</TR>
<TR>
<TH ALIGN=left>Date:</TH>
<TD ALIGN=left>February 13, 2006 (11:31)</TD>
</TR>
<!--
<TR>
<TH ALIGN=left>News Source:</TH>
<TD ALIGN=left></TD>
</TR>
-->
<TR>
<TH ALIGN=left>Comment:</TH>
<TD ALIGN=left>Does the iPod's shuffle really play songs at random?

http://msnbc.msn.com/id/6854309/site/newsweek/

Interesting take here by a Newsweek journalist. I guess we could see how 'random' the internal number generator is by using the machine over a period of time and then formulating probabilities for certain songs being repeated within that time. The Shuffle is clearly has a uniform probability distribution of 1/n where n is the number of songs stored on the iPod. Surely we will get certain anomalies - a song being played 3 times in an hour - but what exactly are the chances of a 10+ songs being played 5+ times in say 5 hours if the iPod holds 20,000 songs. The composite of repetitions can possibly give us an answer to our question.</TD>
</TR>
</TABLE>
<P>

<HR noshade>

<TABLE cellspacing="2" cellpadding = 5 border="0" 
BGCOLOR = "#c9dce1" WIDTH = 550>
<TR >
<TH WIDTH=75 ALIGN=left>Name:</TH>
<TD WIDTH=475 ALIGN=left>Alison</TD>
</TR>
<TR>
<TH ALIGN=left>Date:</TH>
<TD ALIGN=left>February 13, 2006 (10:56)</TD>
</TR>
<!--
<TR>
<TH ALIGN=left>News Source:</TH>
<TD ALIGN=left></TD>
</TR>
-->
<TR>
<TH ALIGN=left>Comment:</TH>
<TD ALIGN=left>Near Earth Asteroid Population Estimate: http://www.sciencemag.org/cgi/content/full/294/5547/1691?maxtoshow=&HITS=10&hits=10&RESULTFORMAT=&fulltext=estimators&searchid=1139845209136_8850&FIRSTINDEX=0&journalcode=sci             In this article, the author works to come up with a estimated population of near-earth asteroids based on 1300 observations. We're interested in knowing 4 properties of NEAs: absolute magnitude, semi-major axis, eccentricity, and inclination. *BUT* it's much easier to detect certain members of this population than others. For example, brighter asteroids (high absolute magnitude) and asteroids closer to Earth are easier to detect. In order to account for this effect, the author makes a model to figure out the probability p of a certain type (abs. mag, semi-major axis, ellipticity and inclination) of asteroid being detected. He then uses x/p (where x is the number of that type of asteroids detected) as an unbiased estimator for the actual number of asteroids of that type.</TD>
</TR>
</TABLE>
<P>

<HR noshade>

<TABLE cellspacing="2" cellpadding = 5 border="0" 
BGCOLOR = "#c9dce1" WIDTH = 550>
<TR >
<TH WIDTH=75 ALIGN=left>Name:</TH>
<TD WIDTH=475 ALIGN=left>Alison</TD>
</TR>
<TR>
<TH ALIGN=left>Date:</TH>
<TD ALIGN=left>February 13, 2006 (10:55)</TD>
</TR>
<!--
<TR>
<TH ALIGN=left>News Source:</TH>
<TD ALIGN=left></TD>
</TR>
-->
<TR>
<TH ALIGN=left>Comment:</TH>
<TD ALIGN=left>Near Earth Asteroid Population Estimate: http://www.sciencemag.org/cgi/content/full/294/5547/1691?maxtoshow=&HITS=10&hits=10&RESULTFORMAT=&fulltext=estimators&searchid=1139845209136_8850&FIRSTINDEX=0&journalcode=sci             In this article, the author works to come up with a estimated population of near-earth asteroids based on 1300 observations. We're interested in knowing 4 properties of NEAs: absolute magnitude, semi-major axis, eccentricity, and inclination. *BUT* it's much easier to detect certain members of this population than others. For example, brighter asteroids (high absolute magnitude) and asteroids closer to Earth are easier to detect. In order to account for this effect, the author makes a model to figure out the probability p of a certain type (abs. mag, semi-major axis, ellipticity and inclination) of asteroid being detected. He then uses x/p (where x is the number of that type of asteroids detected) as an unbiased estimator for the actual number of asteroids of that type.</TD>
</TR>
</TABLE>
<P>

<HR noshade>

<TABLE cellspacing="2" cellpadding = 5 border="0" 
BGCOLOR = "#c9dce1" WIDTH = 550>
<TR >
<TH WIDTH=75 ALIGN=left>Name:</TH>
<TD WIDTH=475 ALIGN=left>Neha</TD>
</TR>
<TR>
<TH ALIGN=left>Date:</TH>
<TD ALIGN=left>February 13, 2006 (09:03)</TD>
</TR>
<!--
<TR>
<TH ALIGN=left>News Source:</TH>
<TD ALIGN=left></TD>
</TR>
-->
<TR>
<TH ALIGN=left>Comment:</TH>
<TD ALIGN=left>Its almost tax season and with that comes the fear of being audited.  Last year the IRS audited 1.2 million 2004 tax returns.  This is up 20% from the previous years audits.  The article states that the for someone who earns over 100,000 the odds of being audited is 1 in 63.  However the overall odds of being audited is 1 in 107.  Most of these problems are not severe violations of tax laws but rather are giving the incorrect social security number or forgetting to sign a form. 
Source:
http://www.sunherald.com/mld/macon/business/13851709.htm?source=rss&channel=macon_business</TD>
</TR>
</TABLE>
<P>

<HR noshade>

<TABLE cellspacing="2" cellpadding = 5 border="0" 
BGCOLOR = "#c9dce1" WIDTH = 550>
<TR >
<TH WIDTH=75 ALIGN=left>Name:</TH>
<TD WIDTH=475 ALIGN=left>Pauline</TD>
</TR>
<TR>
<TH ALIGN=left>Date:</TH>
<TD ALIGN=left>February 12, 2006 (18:54)</TD>
</TR>
<!--
<TR>
<TH ALIGN=left>News Source:</TH>
<TD ALIGN=left></TD>
</TR>
-->
<TR>
<TH ALIGN=left>Comment:</TH>
<TD ALIGN=left>According to the 2nd annual Scotiabank Valentine's Day Consumer Spending Study, Canadians will be spending more money for this Valentine's day than they did last year. On average, Canadians plan to spend $91, up from $61 last year. Men continue to spend the
most, paying an average of $120 for gifts while women will spend an average of $62, $15 more than last year. Married men are planning to spend the most this Valentine's Day as they plan on spending an average of $130 whereas married women are planning to spend an average of $66.  Girlfriends plan to spend an average of $118 on their girlfriends while boyfriends can expect a gift that averages $66. Survey findings are based on a Decima Research Inc. telephone poll in January 2006. The poll is based on a randomly selected sample of 970 English and French speaking adult Canadians. At the 95% confidence level, statistical error due to sampling is no more than +/-3.1% for the total sample. A question that comes up is how seriously to take into consideration the margin of error. While the statistical error is only at most 3.1%, this number is only meaningful under the assumption that there are no biases influencing their answers. However, there are many other factors that can influence the biasedness of the results such as how the adults are selected and the context in which the telephone survey was conducted. <br>
<a href="http://www.cnw.ca/fr/releases/archive/February2006/08/c0109.html">article link</a>
</TD>
</TR>
</TABLE>
<P>

<HR noshade>

<TABLE cellspacing="2" cellpadding = 5 border="0" 
BGCOLOR = "#c9dce1" WIDTH = 550>
<TR >
<TH WIDTH=75 ALIGN=left>Name:</TH>
<TD WIDTH=475 ALIGN=left>Kasia</TD>
</TR>
<TR>
<TH ALIGN=left>Date:</TH>
<TD ALIGN=left>February 11, 2006 (13:52)</TD>
</TR>
<!--
<TR>
<TH ALIGN=left>News Source:</TH>
<TD ALIGN=left></TD>
</TR>
-->
<TR>
<TH ALIGN=left>Comment:</TH>
<TD ALIGN=left>This past week New York Times published an article about a study that shows little impact of low fat diet in reducing health risks.  The study involved 49,000 women between 50 and 79 years old who were followed for 8 years. After 8 years, women who were assigned low–fat diet had the same rate of breast cancer, colon cancer, strokes and heart attacks as those who were not on the diet. I think this study is interesting because it was very expensive and the results are important, but it still brings in many questions and skepticism. For example, Dr. Dean Ornish, promoter of low-fat diets believes that this study did not give the diet enough time, and that the women did not reduce their fat enough.  Others question reducing all fats from the diet instead of reducing only saturated fats and allowing non-saturated.  Also, the study rises a question of when statistical data become significant: some of the findings shows that LDL cholesterol was slightly higher in women on the higher-fat diet, as well as women on low fat diet has a 9% lower rate of breast cancer, however, these are thought to make no difference to the results of the study.  If more women were taken under the study and the study lasted longer than just 8 years, would the data be significant then? <a href="http://www.nytimes.com/2006/02/08/health/08fat.html?_r=1&oref=slogin">New York Times article</a> , <a href="http://jama.ama-assn.org/cgi/content/full/295/6/629">Journal  of American Medical Association</a></TD>
</TR>
</TABLE>
<P>

<HR noshade>

<TABLE cellspacing="2" cellpadding = 5 border="0" 
BGCOLOR = "#c9dce1" WIDTH = 550>
<TR >
<TH WIDTH=75 ALIGN=left>Name:</TH>
<TD WIDTH=475 ALIGN=left>SK</TD>
</TR>
<TR>
<TH ALIGN=left>Date:</TH>
<TD ALIGN=left>February 11, 2006 (12:25)</TD>
</TR>
<!--
<TR>
<TH ALIGN=left>News Source:</TH>
<TD ALIGN=left></TD>
</TR>
-->
<TR>
<TH ALIGN=left>Comment:</TH>
<TD ALIGN=left>http://www.medpagetoday.com/InfectiousDisease/PublicHealth/tb/2653

This post might make you re-think several habits in your everyday lives. It concerns how some researchers examined the blood samples and nasal and throat swabs of 144 15- to 19-year-olds with confirmed or likely cases of meningococcal diseases and tested for various factors that might increase the risk of illness. They found that following factors increased likelihood for illness: history of illness (odds ratio (OR) of 2.9, which means that subjects with a past history were 2.9 times more likely than other students to be ill, and 95% confidence interval (CI) of 1.4 to 5.9 times), intimate kissing with multiple partners (OR 3.7, 95% CI of 1.7-8.1), being a college student (OR 3.4, 95% CI 1.2-10), and premature birth (24-37 weeks' gestation, or preterm, as opposed to the normal 37-42 week's time) (OR 3.7, 95% CI 1.0-13.5). They also found that students who attended one or more religious ceremonies (OR 0.09, 95% CI 0.02-0.6) and those have been vaccinated (OR 0.12, 95% CI 0.04-0.4) were significantly less likely to have meningococcal diseases. So it appears apparent that there are many types of behaviors, including those on campuses, which could easily spread meningitis and other meningococcal diseases.

For information of gestation and pregnancy, go to:
http://en.wikipedia.org/wiki/Pregnancy</TD>
</TR>
</TABLE>
<P>

<HR noshade>

<TABLE cellspacing="2" cellpadding = 5 border="0" 
BGCOLOR = "#c9dce1" WIDTH = 550>
<TR >
<TH WIDTH=75 ALIGN=left>Name:</TH>
<TD WIDTH=475 ALIGN=left>Seth Zimmerman</TD>
</TR>
<TR>
<TH ALIGN=left>Date:</TH>
<TD ALIGN=left>February 06, 2006 (12:14)</TD>
</TR>
<!--
<TR>
<TH ALIGN=left>News Source:</TH>
<TD ALIGN=left></TD>
</TR>
-->
<TR>
<TH ALIGN=left>Comment:</TH>
<TD ALIGN=left>And one for this week. 

http://sports.espn.go.com/nfl/playoffs05/columns/story?columnist=clayton_john&id=2317573

Every year, the Super Bowl probably produces more aberrant and esoteric statistics than any other single event. This year, for instance, one of the big stories is about how the Steelers have become a different team in the playoffs-- a much better one than they were during the regular season. To me, this seems to be an argument about sampling. Let's say there's some  RV X that represents the number of games the Steelers win. This variable has some "true" distribution, which we can get an idea of by looking at the Steelers' record. The question, then, is whether it makes sense to re-evaluate our pdf for X given the Steelers' playoff performance, or whether these new data fall within the realm of reasonable probability given the old pdf. I'm not sure which would be true-- just developing such a pdf would be a fairly complicated endeavor. But what I will predict is that, no matter what happens, everyone who writes for espn.com (see above link) will be able to say they were right, since they've all made dozens of predictions based on dozens of statistics. 

by the way, her's the link for my post below: http://chance.dartmouth.edu/chancewiki/index.php/Chance_News_13</TD>
</TR>
</TABLE>
<P>

<HR noshade>

<TABLE cellspacing="2" cellpadding = 5 border="0" 
BGCOLOR = "#c9dce1" WIDTH = 550>
<TR >
<TH WIDTH=75 ALIGN=left>Name:</TH>
<TD WIDTH=475 ALIGN=left>Seth Zimmerman</TD>
</TR>
<TR>
<TH ALIGN=left>Date:</TH>
<TD ALIGN=left>February 06, 2006 (12:12)</TD>
</TR>
<!--
<TR>
<TH ALIGN=left>News Source:</TH>
<TD ALIGN=left></TD>
</TR>
-->
<TR>
<TH ALIGN=left>Comment:</TH>
<TD ALIGN=left>Here's one for last week. 

<http://chance.dartmouth.edu/chancewiki/index.php/Chance_News_13>
This article considers a question of sampling somewhat analogous to those we've been studying in class. Specifically, it examines the question of how many studies are sufficient to confirm the effectiveness or ineffectiveness of a medical treatment. Apparenty, doctors conducted 64 studies between 1987 today confirming the effectiveness of a certain drug used for heart surgery. Understandably, the authors of some of the later studies were criticized for not directing their talents towards more contentious issues. A study reported last week, however, has caused doctors to stop using the drug in question because of the danger it poses to patients. Clearly, a surprising result. The question is whether the result was due to superior execution (the study was one of the few not funded by drug companies), or to the statistical inevitability of an aberrant result given a large enough sample of even the most conclusive studies. </TD>
</TR>
</TABLE>
<P>

<HR noshade>

<TABLE cellspacing="2" cellpadding = 5 border="0" 
BGCOLOR = "#c9dce1" WIDTH = 550>
<TR >
<TH WIDTH=75 ALIGN=left>Name:</TH>
<TD WIDTH=475 ALIGN=left>Brian Lloyd</TD>
</TR>
<TR>
<TH ALIGN=left>Date:</TH>
<TD ALIGN=left>February 06, 2006 (12:07)</TD>
</TR>
<!--
<TR>
<TH ALIGN=left>News Source:</TH>
<TD ALIGN=left></TD>
</TR>
-->
<TR>
<TH ALIGN=left>Comment:</TH>
<TD ALIGN=left>http://www.newyorker.com/fact/content/articles/060213fa_fact

The above article makes reference to the normal curve when discussing the distribution of problems within the LAPD police force.  After the Rodney King incident, people wanted to know the distribution of hateful acts throughout the police force.  Many assumed it would be normal, with a few officers engaging in no bad acts, most engaging in a few, with a small percentage engaging in quite a lot.  However, they found that this was not the case.  Only a small percentage of the police force (1800 out of 8500) had any sort of complaint against them in the given period.  So, the distribution would look more like a hockey stick, according to the article, with most of the population at zero.  Statisticians call this a "power law" distribution.

Given more information, it would be possible to see if the complaints follow a Poisson distribution.  The article states that 1400 of the 1800 officers against whom complaints were raised had only 1 or 2 complaints, 183 had four or more, and 1 even had 16.  Given more exact numbers, the Poisson distribution may prove useful.</TD>
</TR>
</TABLE>
<P>

<HR noshade>

<TABLE cellspacing="2" cellpadding = 5 border="0" 
BGCOLOR = "#c9dce1" WIDTH = 550>
<TR >
<TH WIDTH=75 ALIGN=left>Name:</TH>
<TD WIDTH=475 ALIGN=left>Chetan</TD>
</TR>
<TR>
<TH ALIGN=left>Date:</TH>
<TD ALIGN=left>February 06, 2006 (12:07)</TD>
</TR>
<!--
<TR>
<TH ALIGN=left>News Source:</TH>
<TD ALIGN=left></TD>
</TR>
-->
<TR>
<TH ALIGN=left>Comment:</TH>
<TD ALIGN=left>Really cool article about a futures market in Superbowl Tickets:

http://www.wired.com/wired/archive/14.01/posts.html?pg=4

The concept is similar to regular options. As the team advances, the price of the contract and the ticket converges. So, using statistical analysis, you can determine the optimum price for a contract several months in advance based on your idea of the probability of your team actually making it to the Superbowl. A similar article in the NYT stated that the prices closely follow odds on http://www.sportsbetting.com/ as the season goes on, so making an arbitrate profit is somewhat difficult. As the market gets larger and the participants' personal reckoning of winning probabilities gets better, prices will go down and the market will become more liquid. Perhaps someday you'll be able to purchase contracts several years ahead of the game....</TD>
</TR>
</TABLE>
<P>

<HR noshade>

<TABLE cellspacing="2" cellpadding = 5 border="0" 
BGCOLOR = "#c9dce1" WIDTH = 550>
<TR >
<TH WIDTH=75 ALIGN=left>Name:</TH>
<TD WIDTH=475 ALIGN=left>Tom</TD>
</TR>
<TR>
<TH ALIGN=left>Date:</TH>
<TD ALIGN=left>February 06, 2006 (12:03)</TD>
</TR>
<!--
<TR>
<TH ALIGN=left>News Source:</TH>
<TD ALIGN=left></TD>
</TR>
-->
<TR>
<TH ALIGN=left>Comment:</TH>
<TD ALIGN=left>I read an interesting article regarding the occurence of snack food vending machines in Pennsylvania schools. The article showed that children who have earlier lunch periods are more likely to buy food a la carte (P<.001). Moreover, the survey also looked at the numbers of vending machines in these schools, finding a mean to be 5.9 and the standard deviation to be 4.3. According to the article that I read, the survey did not try to determine a correlation between the number of students not purchasing school lunches and increased numbers of vending machines, but it would be interesting to see what type of regression would fit to this data.

http://www.medpagetoday.com/Pediatrics/DietNutrition/tb/2615</TD>
</TR>
</TABLE>
<P>

<HR noshade>

<TABLE cellspacing="2" cellpadding = 5 border="0" 
BGCOLOR = "#c9dce1" WIDTH = 550>
<TR >
<TH WIDTH=75 ALIGN=left>Name:</TH>
<TD WIDTH=475 ALIGN=left>Pauline</TD>
</TR>
<TR>
<TH ALIGN=left>Date:</TH>
<TD ALIGN=left>February 05, 2006 (23:09)</TD>
</TR>
<!--
<TR>
<TH ALIGN=left>News Source:</TH>
<TD ALIGN=left></TD>
</TR>
-->
<TR>
<TH ALIGN=left>Comment:</TH>
<TD ALIGN=left>A study of about 72,000 veterans in Connecticut suggests that screening for prostate cancer using a PSA test does not appear to mortality. The American Cancer Society and American Urological Association have recommended PSA Screening for prostate cancer in men over 50. However, the American College of Physicians recommend the discussion of benefits and risks of the screening and the U.S. Preventive Services Task Force has not found enough evidence to recommend the screening. In order to explain the disparity, Dr. Concato and his colleagues looked at the database of 10 Veteran Affairs hospitals in Connecticut. The results they found were: Of the men who died of cancer, 14% were screened with PSA in comparison to the 13% of the men in the control group. PSA screening did not affect all-cause mortality. The odds ratio was 1.08 with a 95% confidence interval from 0.71 to 1.64. Furthermore, PSA or digital rectal examination screening or both had no effect on cause-specific mortality with an odds ratio of 1.13 with a 95% confidence interval from 0.63 to 2.06.<br>
<a href="http://www.medpagetoday.com/tbprint.cfm?tbid=2453">article link</a></TD>
</TR>
</TABLE>
<P>

<HR noshade>

<TABLE cellspacing="2" cellpadding = 5 border="0" 
BGCOLOR = "#c9dce1" WIDTH = 550>
<TR >
<TH WIDTH=75 ALIGN=left>Name:</TH>
<TD WIDTH=475 ALIGN=left>Neha</TD>
</TR>
<TR>
<TH ALIGN=left>Date:</TH>
<TD ALIGN=left>February 05, 2006 (20:50)</TD>
</TR>
<!--
<TR>
<TH ALIGN=left>News Source:</TH>
<TD ALIGN=left></TD>
</TR>
-->
<TR>
<TH ALIGN=left>Comment:</TH>
<TD ALIGN=left>The prize is 125 million pounds not 125 pounds!</TD>
</TR>
</TABLE>
<P>

<HR noshade>

<TABLE cellspacing="2" cellpadding = 5 border="0" 
BGCOLOR = "#c9dce1" WIDTH = 550>
<TR >
<TH WIDTH=75 ALIGN=left>Name:</TH>
<TD WIDTH=475 ALIGN=left>Neha</TD>
</TR>
<TR>
<TH ALIGN=left>Date:</TH>
<TD ALIGN=left>February 05, 2006 (20:47)</TD>
</TR>
<!--
<TR>
<TH ALIGN=left>News Source:</TH>
<TD ALIGN=left></TD>
</TR>
-->
<TR>
<TH ALIGN=left>Comment:</TH>
<TD ALIGN=left> Ticket sales have increased 1200% in the UK at the prospect of winning 125 pounds.  The actual odds of winning are 76 million to 1.  The article asks the question why are so many people willing to pay 1.5 pounds to play a game they will most likely loose.  A Psychology professor, Alastair Ross specializes in interpreting gambling and risk claims that it is because gamblers do not really understand improbability of winning.  Dr. Ross puts the chances of winning into context " a weekly player of the national lottery could only expect to win a jackpot if they lived to be 300,000 years old."  Another factor is that people believe that playing more often will increase their odds. This is not true since the probability of winning is the same each time it is played.  Dr. Ross explains the increase in online gambling and poker as a way for risk averse people to get excitement at a relatively low cost.  He states that increased gambling is the result of an increasingly risk averse society.
http://news.bbc.co.uk/2/hi/uk_news/magazine/4673280.stm</TD>
</TR>
</TABLE>
<P>

<HR noshade>

<TABLE cellspacing="2" cellpadding = 5 border="0" 
BGCOLOR = "#c9dce1" WIDTH = 550>
<TR >
<TH WIDTH=75 ALIGN=left>Name:</TH>
<TD WIDTH=475 ALIGN=left>Kasia</TD>
</TR>
<TR>
<TH ALIGN=left>Date:</TH>
<TD ALIGN=left>February 04, 2006 (18:03)</TD>
</TR>
<!--
<TR>
<TH ALIGN=left>News Source:</TH>
<TD ALIGN=left></TD>
</TR>
-->
<TR>
<TH ALIGN=left>Comment:</TH>
<TD ALIGN=left>Review of an article from the Canadian Medical Association Journal, describes that people with diabetes who are talled are more likely to need lower limb amputation.  A study involving 93,484 people was conducted to check whether taller people are in bigger risk of  amputations.  Among the various results the study showed that 10 cm increment in height was highly associated with a higher risk of amputation.  The odds ratio, adjusted factors such as age, sex, smoking status, length of diabetes, hypertension, and type of disease, was 1.16, with a 95% confidence interval from 1.03 to 1.32.  Also other results were given in terms of 95% confidence interval.  Although the study had its limitations it show that height can be a risk factor for amputations for people with diabetes.   Link to the article: http://www.medpagetoday.com/Endocrinology/Diabetes/tb/2585</TD>
</TR>
</TABLE>
<P>

<HR noshade>

<TABLE cellspacing="2" cellpadding = 5 border="0" 
BGCOLOR = "#c9dce1" WIDTH = 550>
<TR >
<TH WIDTH=75 ALIGN=left>Name:</TH>
<TD WIDTH=475 ALIGN=left>SK</TD>
</TR>
<TR>
<TH ALIGN=left>Date:</TH>
<TD ALIGN=left>February 04, 2006 (11:45)</TD>
</TR>
<!--
<TR>
<TH ALIGN=left>News Source:</TH>
<TD ALIGN=left></TD>
</TR>
-->
<TR>
<TH ALIGN=left>Comment:</TH>
<TD ALIGN=left>http://newsinfo.iu.edu/news/page/normal/2869.html

Scientists at the Indiana University School of Informatics used predictions and confidence intervals to build models that better predict the outbreak of epidemics along air transport routes, based on data from airlines and census information (for disease patterns). They confirmedd that air transportation plays a major role in the spread in diseases like SARS or pandemic influenzas.  These researchers plan to perform similar work with road and rail transportation, and to see how those are related to air travel.</TD>
</TR>
</TABLE>
<P>

<HR noshade>

<TABLE cellspacing="2" cellpadding = 5 border="0" 
BGCOLOR = "#c9dce1" WIDTH = 550>
<TR >
<TH WIDTH=75 ALIGN=left>Name:</TH>
<TD WIDTH=475 ALIGN=left>Dave Raines</TD>
</TR>
<TR>
<TH ALIGN=left>Date:</TH>
<TD ALIGN=left>February 02, 2006 (15:56)</TD>
</TR>
<!--
<TR>
<TH ALIGN=left>News Source:</TH>
<TD ALIGN=left></TD>
</TR>
-->
<TR>
<TH ALIGN=left>Comment:</TH>
<TD ALIGN=left>From the new york times

the article talks about people being naturally risk averse. A majority of people would accept $1,000 to a 50%  chance of $4,000. It goes on to break down the risk taking by gender and other factors for instance, a test was given which tested intelligence in addition to patience and thoughtfulness. high scoring men were more risk-loving. With 80% of high scoring men accepted 15% chance of a million dollars instead of a sure $500. this is compared to 38% of high scoring women, 40% of low scoring days, and 25% of low scoring women.</TD>
</TR>
</TABLE>
<P>

<HR noshade>

<TABLE cellspacing="2" cellpadding = 5 border="0" 
BGCOLOR = "#c9dce1" WIDTH = 550>
<TR >
<TH WIDTH=75 ALIGN=left>Name:</TH>
<TD WIDTH=475 ALIGN=left>Alex</TD>
</TR>
<TR>
<TH ALIGN=left>Date:</TH>
<TD ALIGN=left>February 01, 2006 (21:38)</TD>
</TR>
<!--
<TR>
<TH ALIGN=left>News Source:</TH>
<TD ALIGN=left></TD>
</TR>
-->
<TR>
<TH ALIGN=left>Comment:</TH>
<TD ALIGN=left>Derivation of the Pythagorean Won-Loss Formula in Baseball

<p>
Here's an example of work of a colleague of mine at Brown using a certain pdf (the Weibull, a generalization of the gamma we did today) to explain a rule of thumb from baseball. It's a pretty thorough analysis, including studying the real baseball data. This is the style of projects you could do in this class.
Note that lots of gamma-function integrals are used - yes that nasty stuff we just did is useful!
Here's the link:
<a href="http://arxiv.org/abs/math.ST/0509698">Miller's paper</a></TD>
</TR>
</TABLE>
<P>

<HR noshade>

<TABLE cellspacing="2" cellpadding = 5 border="0" 
BGCOLOR = "#c9dce1" WIDTH = 550>
<TR >
<TH WIDTH=75 ALIGN=left>Name:</TH>
<TD WIDTH=475 ALIGN=left>Mark Christman</TD>
</TR>
<TR>
<TH ALIGN=left>Date:</TH>
<TD ALIGN=left>February 01, 2006 (11:11)</TD>
</TR>
<!--
<TR>
<TH ALIGN=left>News Source:</TH>
<TD ALIGN=left></TD>
</TR>
-->
<TR>
<TH ALIGN=left>Comment:</TH>
<TD ALIGN=left>http://www.onwallstreet.com/article.cfm?articleid=3217

This story outlines the peculiarities of hedge funds, which are currently becoming more and more available to non-institutional investors.  New products offered by hedgefunds (a fund of hedgefunds) are becoming available to smaller investors.  This is not necessarily a good thing.  Because of lax regulations, hedge funds have considerable freedom in reporting historical returns.  Banks, with high regulations, are forced to disclose all information on failed and successful investment funds, which genereates a normal distribution of risk versus return.  Because hedge funds have an incentive to overreport positive results, they appear to have a leptokurtic risk vs return distribution, which implies that higher returns at low levels of risk.  However, this may just be a result of the bias in investing, and individual investors may end up losing there shirts if they begin taking large postions in these funds.</TD>
</TR>
</TABLE>
<P>

<HR noshade>

<TABLE cellspacing="2" cellpadding = 5 border="0" 
BGCOLOR = "#c9dce1" WIDTH = 550>
<TR >
<TH WIDTH=75 ALIGN=left>Name:</TH>
<TD WIDTH=475 ALIGN=left>Neha</TD>
</TR>
<TR>
<TH ALIGN=left>Date:</TH>
<TD ALIGN=left>January 31, 2006 (19:46)</TD>
</TR>
<!--
<TR>
<TH ALIGN=left>News Source:</TH>
<TD ALIGN=left></TD>
</TR>
-->
<TR>
<TH ALIGN=left>Comment:</TH>
<TD ALIGN=left>http://abcnews.go.com/Health/wireStory?id=1562166
Lupus, is a disease in which a person's immune system believes that healthy organs and cells pose a threat to the body, affects 1.5 million people in America, 90% of which are women.  5% of all lupus patients have severe lupus and cannot respond to conventional treatments.  A new study showed that using stem cell transplant from a patients own marrow could provide a cure for severe lupus.  In a study of 48 patients, 33 of them continued to be symptom free 7 years after treatment.  With this treatment the probability of disease free survival for the next 5 years was 50%.   </TD>
</TR>
</TABLE>
<P>

<HR noshade>

<TABLE cellspacing="2" cellpadding = 5 border="0" 
BGCOLOR = "#c9dce1" WIDTH = 550>
<TR >
<TH WIDTH=75 ALIGN=left>Name:</TH>
<TD WIDTH=475 ALIGN=left>Matt</TD>
</TR>
<TR>
<TH ALIGN=left>Date:</TH>
<TD ALIGN=left>January 30, 2006 (12:23)</TD>
</TR>
<!--
<TR>
<TH ALIGN=left>News Source:</TH>
<TD ALIGN=left></TD>
</TR>
-->
<TR>
<TH ALIGN=left>Comment:</TH>
<TD ALIGN=left>http://www.cnn.com/2006/HEALTH/01/30/health.alcohol.reut/index.html.

A new study by the Agency for Research on Cancer has found that excessive alcohol consumption raises the risk of cancer. Mouth, larynx, esophagus, liver, colon, and breast cancer were found to have a direct link with alcohol consumption. The Who estimates that in developed countries 185,000 men and 142,000 women died due to alcohol. Conversely alcohol saved 71,000 male deaths and 277,000 female deaths. This reflects the benefits of small levels of alcohol consumption on cardiovascular disease. In the developing world where cardiovascular disease is less of a factor, alcohol was linked to 1.52 million male deaths and 301,000 female ones. According to Dr. Paolo Boffetta “Alcohol is probably the main factor responsible for increased risk of head and neck cancer recorded in various countries, particularly in central and east Europe”. Clearly there is an optimization problem happening here. Some alcohol consumption prevents cardiovascular disease but these effects are diminishing. Conversely it is likely that alcohol’s effect on cancer increases as consumption increases. Thus Researchers recommended that males drink two glasses maximum per day while women limit themselves to one drink per day.  </TD>
</TR>
</TABLE>
<P>

<HR noshade>

<TABLE cellspacing="2" cellpadding = 5 border="0" 
BGCOLOR = "#c9dce1" WIDTH = 550>
<TR >
<TH WIDTH=75 ALIGN=left>Name:</TH>
<TD WIDTH=475 ALIGN=left>Pauline</TD>
</TR>
<TR>
<TH ALIGN=left>Date:</TH>
<TD ALIGN=left>January 29, 2006 (19:29)</TD>
</TR>
<!--
<TR>
<TH ALIGN=left>News Source:</TH>
<TD ALIGN=left></TD>
</TR>
-->
<TR>
<TH ALIGN=left>Comment:</TH>
<TD ALIGN=left>Recently, psychologists have been researching if the human brain itself might be a Bayesian-reasoning machine. Many researchers have conducted laboratory experiments that have yielded positive results, but now the focus has shifted on whether the brain deals with everyday judgments in a Bayesian manner. Thomas Griffiths of Brown University and Joshua Tenenbaum of MIT conducted an experiment by giving individual nuggets of information to each of the participants in their study and asking them to draw a general conclusion. For example, the participants were asked about the number of lines in a poem (given how far into the poem a single line is), the time it takes to bake a cake (given how long it has already been in the oven), and the total length of the term that would be served by an American congressman (given how long he has already been in the House of Representatives). All of these things have well-established probability distributions, and all of them were predicted accurately by the participants from lone pieces of data. Griffiths and Tenenbaum have shown the vast range of distributions the mind can cope with such as normal, complex, and irregular distributions. This data supports the idea that the Bayesian capacity to draw inferences from sparse data could be crucial to the way the mind perceives the world, plans actions, comprehends and learns languages, reasons from correlation to causation, and even understands the goals and beliefs of other minds.<br>
<a href="http://www.cfo.com/printable/article.cfm/5375519?f=options">article link</a></TD>
</TR>
</TABLE>
<P>

<HR noshade>

<TABLE cellspacing="2" cellpadding = 5 border="0" 
BGCOLOR = "#c9dce1" WIDTH = 550>
<TR >
<TH WIDTH=75 ALIGN=left>Name:</TH>
<TD WIDTH=475 ALIGN=left>Kasia</TD>
</TR>
<TR>
<TH ALIGN=left>Date:</TH>
<TD ALIGN=left>January 28, 2006 (18:25)</TD>
</TR>
<!--
<TR>
<TH ALIGN=left>News Source:</TH>
<TD ALIGN=left></TD>
</TR>
-->
<TR>
<TH ALIGN=left>Comment:</TH>
<TD ALIGN=left>An article talks about a study of data using Poisson analysis to check for the effects of heat and mortality from heat in low-income countries as opposed to high-income countries.  Data of heatwaves, mortalities and their causes in 3 cities of different wealth levels: Delhi, Sao Paulo and London,  were analyzed using Possoin models.   It revealed vast differences between low and high income countries heat deaths, especially in the length of time of the effects.  The risk of death associated with heat was found to be 2.4% per degree greater than 20 degrees Celcius in Delhi, 0.8% in Sao Paulo and -1.6% in London.  The conclusion was  that people in  low income countries are more susceptible to the effects of heat such as infections and various diseases than people in high income countries.  I think this article is an example of how Poisson distribution can be used to analyze and interpret data.  http://www.co2science.org/scripts/CO2ScienceB2C/articles/V9/N3/B2.jsp</TD>
</TR>
</TABLE>
<P>

<HR noshade>

<TABLE cellspacing="2" cellpadding = 5 border="0" 
BGCOLOR = "#c9dce1" WIDTH = 550>
<TR >
<TH WIDTH=75 ALIGN=left>Name:</TH>
<TD WIDTH=475 ALIGN=left>SK</TD>
</TR>
<TR>
<TH ALIGN=left>Date:</TH>
<TD ALIGN=left>January 28, 2006 (00:42)</TD>
</TR>
<!--
<TR>
<TH ALIGN=left>News Source:</TH>
<TD ALIGN=left></TD>
</TR>
-->
<TR>
<TH ALIGN=left>Comment:</TH>
<TD ALIGN=left>http://www.edubourse.com/finance/actualites.php?idActus=25677

The Directorate General for Economic and Financial Affairs of the European Commission developed a model to project GDP growth projections in the euro area up to the second quarter of 2006. The model is a dynamic factor model, which means that several factors that drive the euro area business cycle from a dataset of ~2000 time series are used to make projections of GDP. Because of the uncertainty that is inherent in making predictions, there are no point estimates, only confidence interval ranges (1 standard deviation in each direction). The approximations are based on an experiment that ran simulations based on the existing data. This is done under the assumption that forecasting errors follow a normal distribution. This system estimated 0.4-0.8% growth in the 1st quarter of 2006 and 0.4-0.9% in the 2nd quarter. This study shows how normal distributions and standard deviations can be used to make economic predictions given existing data.</TD>
</TR>
</TABLE>
<P>

<HR noshade>

<TABLE cellspacing="2" cellpadding = 5 border="0" 
BGCOLOR = "#c9dce1" WIDTH = 550>
<TR >
<TH WIDTH=75 ALIGN=left>Name:</TH>
<TD WIDTH=475 ALIGN=left>Ralph Callaway</TD>
</TR>
<TR>
<TH ALIGN=left>Date:</TH>
<TD ALIGN=left>January 23, 2006 (16:34)</TD>
</TR>
<!--
<TR>
<TH ALIGN=left>News Source:</TH>
<TD ALIGN=left></TD>
</TR>
-->
<TR>
<TH ALIGN=left>Comment:</TH>
<TD ALIGN=left><a href="http://www.uruknet.info/?p=m19423&l=i&size=1&hd=0">30,000? No. 100,000? No. How Many Iraqis Have Died since the Us invasion in 2003?</a>
<p>
This article takes a look at the variety of methods which have been used to generate statistics of how many Iraqis have died as a result of the U.S. occupation of Iraq.  The article is primarily a reaction to President Bush's off-hand quote of "30,000 more or less."
<p>
The article remarks how figures from iraqbodycount.org, gives an undeserved air of scientific precision due to its inclusion of minimum and maximum figures of 27787 and 31317 respectively.
<p>
A more reliable method would be the random sampling used in epidemoligical study by John Hopkins University to model consquent disease and starvation in the Congolese civil war.
<p>
One such study for Iraq had a 95% CI of 8,000 to 194,000 deaths.  In spite of the studies more rigorous statistical methods, it has drawn much more criticism than the less descriptive iraqbodycount.org study.
<p>
However, even this study was attacked in the article due to its antiquated "figure imposed ... by the fascist bell curve." Going on to remark that "The eugenicists of the 1920s were much enamored of Gaussian methodology."  Hmm, not sure if thats quite fair or a logically sound argument.  But the statistician went on to explain that "distribution free" or "non parametric" methods were much better suited due to the strong right skew of the iraq data.  Using these methods the article cited a 95% CI of 53k to 279k deaths in iraq, a much higher figure reflecting the strong right skew of the data.
<p>
While the article has a lot of ideological biases it highlights the important question of how to best model real world data.  Furthermore it emphasizes that natural world phenomena rarely (not sure of the validity of this) are modeled by Gaussian distributions.  In fact "(A $200,000 prize offered in the 1920s for anyone who could provide rigorous evidence of natural occurens of the [gaussian]
 curve remains unclaimed)" (Again not sure how to evaluate the importance of that statement).</TD>
</TR>
</TABLE>
<P>

<HR noshade>

<TABLE cellspacing="2" cellpadding = 5 border="0" 
BGCOLOR = "#c9dce1" WIDTH = 550>
<TR >
<TH WIDTH=75 ALIGN=left>Name:</TH>
<TD WIDTH=475 ALIGN=left>Tom Healy</TD>
</TR>
<TR>
<TH ALIGN=left>Date:</TH>
<TD ALIGN=left>January 23, 2006 (12:10)</TD>
</TR>
<!--
<TR>
<TH ALIGN=left>News Source:</TH>
<TD ALIGN=left></TD>
</TR>
-->
<TR>
<TH ALIGN=left>Comment:</TH>
<TD ALIGN=left>My article this week shows how statistics can in some ways be applied to real world examples. The article I chose was from the NY times, and it was about how real-estate developers are creating communities for special groups of people, specifically baby-boomers in this article. Certain developers saw that the increased number of people aged 55+ would create a need for more 'active' communities for empty nesters, and they chose to fill these needs. It would be very interesting to see how real-estate planners could look at population trends in the census data to predetermine what types of homes and communities they should create in order to maximize their profits. In the article provided, the company was incredibly successful for providing this niche, and it seems as though other developers could do the same for other groups of people.

http://www.nytimes.com/2006/01/22/realestate/22njzo.html</TD>
</TR>
</TABLE>
<P>

<HR noshade>

<TABLE cellspacing="2" cellpadding = 5 border="0" 
BGCOLOR = "#c9dce1" WIDTH = 550>
<TR >
<TH WIDTH=75 ALIGN=left>Name:</TH>
<TD WIDTH=475 ALIGN=left>Mark Christman</TD>
</TR>
<TR>
<TH ALIGN=left>Date:</TH>
<TD ALIGN=left>January 23, 2006 (12:06)</TD>
</TR>
<!--
<TR>
<TH ALIGN=left>News Source:</TH>
<TD ALIGN=left></TD>
</TR>
-->
<TR>
<TH ALIGN=left>Comment:</TH>
<TD ALIGN=left>my article is on the front page of USA Today, Monday January 23.</TD>
</TR>
</TABLE>
<P>

<HR noshade>

<TABLE cellspacing="2" cellpadding = 5 border="0" 
BGCOLOR = "#c9dce1" WIDTH = 550>
<TR >
<TH WIDTH=75 ALIGN=left>Name:</TH>
<TD WIDTH=475 ALIGN=left>Mark Christman</TD>
</TR>
<TR>
<TH ALIGN=left>Date:</TH>
<TD ALIGN=left>January 23, 2006 (12:05)</TD>
</TR>
<!--
<TR>
<TH ALIGN=left>News Source:</TH>
<TD ALIGN=left></TD>
</TR>
-->
<TR>
<TH ALIGN=left>Comment:</TH>
<TD ALIGN=left>Runway Reports OFTEN Unreliable

This posting is mainly inspired by the fact that I absolutely hate it when the newspaper exagerates scant occurances.  Since 1995, there have been 42 noteworthy complaints about misinformation about airport runway conditions.  Let's make some assumptions and see how "often" they occur.  Let's say there are 20 major airports that actually receive temps cold enough to freeze up runways in the winter, and let's average the period to 3 months.  In that span, lets say that there are 4 hours where the temperature is particularly low each day.  If each airpor can handle 2 incoming flights at a time every 15 minutes, that is 8 flights an hour, or 32 flights in the given "freeze-frame,"  32 flights times 90 days is 2970 flights a year, times 10 years, is 29,700 flights.  That makes the failure rate of the current advisory system at a shade over .1%.  1 in 1000 is not very often.</TD>
</TR>
</TABLE>
<P>

<HR noshade>

<TABLE cellspacing="2" cellpadding = 5 border="0" 
BGCOLOR = "#c9dce1" WIDTH = 550>
<TR >
<TH WIDTH=75 ALIGN=left>Name:</TH>
<TD WIDTH=475 ALIGN=left>Seth Zimmerman</TD>
</TR>
<TR>
<TH ALIGN=left>Date:</TH>
<TD ALIGN=left>January 23, 2006 (10:53)</TD>
</TR>
<!--
<TR>
<TH ALIGN=left>News Source:</TH>
<TD ALIGN=left></TD>
</TR>
-->
<TR>
<TH ALIGN=left>Comment:</TH>
<TD ALIGN=left>http://chance.dartmouth.edu/chancewiki/index.php/Chance_News_12

This isn't especially current, but it's still fairly interesting. The article describes the ways in which Bayesian statistics are more appropriate for modeling human cognition than frequentist statistics. That is, while frequentist statistics allow us to draw very accurate inferences from extremely large and balanced samples, bayesian statistics allow for good guesses based on limited data sets, provided the background understanding (operationalized as the posterior distribution) is relatively accurate. Evidence for the second interpretation comes from simple prediction experiments. For instance, people who are asked to guess how much longer a 60 year old man will live can answer fairly accurately, presumably because they have some prior distributional understanding of how long men tend to live. 

Also, I came across this other article. It reports how government statisticians authoring a report on oil prices accidentally added five zeroes to one of their data points-- they reported 800,000,000,000 cubic feet of oil when they meant 8,000,000. Needless to say, this threw off their analysis a bit. 

http://www.nytimes.com/2006/01/23/business/23leasesbox.html</TD>
</TR>
</TABLE>
<P>

<HR noshade>

<TABLE cellspacing="2" cellpadding = 5 border="0" 
BGCOLOR = "#c9dce1" WIDTH = 550>
<TR >
<TH WIDTH=75 ALIGN=left>Name:</TH>
<TD WIDTH=475 ALIGN=left>Neha</TD>
</TR>
<TR>
<TH ALIGN=left>Date:</TH>
<TD ALIGN=left>January 22, 2006 (23:47)</TD>
</TR>
<!--
<TR>
<TH ALIGN=left>News Source:</TH>
<TD ALIGN=left></TD>
</TR>
-->
<TR>
<TH ALIGN=left>Comment:</TH>
<TD ALIGN=left>An ABC news article revealed a greater inaccuracy in HIV swab tests then first believe.  After comparing the results of blood work and swap tests, San Francisco health officials discovered that 25% of the patients were said to have the disease when blood work proved otherwise.  The swap test has not had any cases in San Francisco of giving a negative reading when the patient was HIV positive.  The company who produces the test, Orasure Technologies, states that data collection of thousands of samples across different cities proved that the results were reliable.  "One explanation for the spate of false positives might be that there is something unique about the San Francisco group, such as a high number of people with hepatitis, that may unexpectedly interfere with the test results, said Deanne Sykes, a research scientist for the California Office of AIDS."
Source:
http://abcnews.go.com/Health/wireStory?id=1391639&page=1</TD>
</TR>
</TABLE>
<P>

<HR noshade>

<TABLE cellspacing="2" cellpadding = 5 border="0" 
BGCOLOR = "#c9dce1" WIDTH = 550>
<TR >
<TH WIDTH=75 ALIGN=left>Name:</TH>
<TD WIDTH=475 ALIGN=left>Pauline</TD>
</TR>
<TR>
<TH ALIGN=left>Date:</TH>
<TD ALIGN=left>January 22, 2006 (19:56)</TD>
</TR>
<!--
<TR>
<TH ALIGN=left>News Source:</TH>
<TD ALIGN=left></TD>
</TR>
-->
<TR>
<TH ALIGN=left>Comment:</TH>
<TD ALIGN=left>This article talks about how irrational people are when making decisions involving risk or the possibility of loss. Behavior financial researchers explain that when giving the choice of being 100% certain of getting $1000 or having a 50:50 chance of getting $2500, most people would choose the first case. However, in calculating the expected values for each situation, the payoff is actually higher in the 2nd choice ($1250). Nonetheless, the 50% chance of getting nothing scares people into thinking that this case is not worthwhile. Similarly, when considering the situation of losing $1000 or having a 50:50 chance of losing $2500, most people choose the 2nd situation. According to behavior psychologists, Amos Tversky and Daniel Kahneman, people are not only irrational but they are consistently irrational.

<a href="http://www.theage.com.au/news/money/kidding-yourself-how-to-limit-the-risks/2006/01/20/1137734149114.html?page=fullpage#contentSwap2">article link</a></TD>
</TR>
</TABLE>
<P>

<HR noshade>

<TABLE cellspacing="2" cellpadding = 5 border="0" 
BGCOLOR = "#c9dce1" WIDTH = 550>
<TR >
<TH WIDTH=75 ALIGN=left>Name:</TH>
<TD WIDTH=475 ALIGN=left>Kasia</TD>
</TR>
<TR>
<TH ALIGN=left>Date:</TH>
<TD ALIGN=left>January 21, 2006 (17:51)</TD>
</TR>
<!--
<TR>
<TH ALIGN=left>News Source:</TH>
<TD ALIGN=left></TD>
</TR>
-->
<TR>
<TH ALIGN=left>Comment:</TH>
<TD ALIGN=left>An article from the Economist: 
http://economist.com/science/displaystory.cfm?story_id=5354696&no_na_tran=1
The first day of class we talked a little about Frequentist Statistics which is pretty much what we’ve been studying so far with p being a probability of an event and claculating probabilities of events related.  We also talked about Bayesian Inference where probability is unknown and so every new event gives new information and changes our probability function.  The article talks about the comeback of Bayesian ideas among computer software designers who are trying to design software that would with “human-like intelligence”.   Aparantely, there are speculations that human brain  may be working in a Bayesian way drawing conclusions from very sparse data.   The article discusses Bayiesian distrubution as well as the importance of good prior information on the Bayesian prediction.  Researchers have also conducted a study were they gave participants a piece of information and asked to predict something.  Since this information already had well developed probability distributions it was possible to check whether the participants predicted accurately.  The participants usually predicted correctly.
</TD>
</TR>
</TABLE>
<P>

<HR noshade>

<TABLE cellspacing="2" cellpadding = 5 border="0" 
BGCOLOR = "#c9dce1" WIDTH = 550>
<TR >
<TH WIDTH=75 ALIGN=left>Name:</TH>
<TD WIDTH=475 ALIGN=left>SK</TD>
</TR>
<TR>
<TH ALIGN=left>Date:</TH>
<TD ALIGN=left>January 21, 2006 (15:25)</TD>
</TR>
<!--
<TR>
<TH ALIGN=left>News Source:</TH>
<TD ALIGN=left></TD>
</TR>
-->
<TR>
<TH ALIGN=left>Comment:</TH>
<TD ALIGN=left>http://www.usda.gov/wps/portal/!ut/p/_s.7_0_A/7_0_1OB?contentidonly=true&contentid=2006/01/0020.xml
http://abcnews.go.com/Health/story?id=1526829

Although not directly related to the most recent material in the course, this article does refer to something we did a few weeks earlier. After Japanese health authorities found a piece of bone that it deemed risky for BSE (mad cow disease) in a shipment of American beef, it renewed its ban (the first ban, which was repealed six weeks ago, was made after BSE was first found in an American cow in 2003) on US beef imports. In response, the US Department of Agriculture announced that it would start unnanounced inspections at every plant that exports beef. This is similar to acceptance sampling in that a sample is picked for quality checks simply because checking everything would be too costly. Asides the questions regarding inspection methodology, one could also ask: 1) what is the acceptance level? 2) What is the presumed percentage of defected beef? 3) What is the probability that the shipment will be defective given the presumed percentage? A study should look at these questions, since there are many implications for public health.</TD>
</TR>
</TABLE>
<P>

<HR noshade>

<TABLE cellspacing="2" cellpadding = 5 border="0" 
BGCOLOR = "#c9dce1" WIDTH = 550>
<TR >
<TH WIDTH=75 ALIGN=left>Name:</TH>
<TD WIDTH=475 ALIGN=left>Brian</TD>
</TR>
<TR>
<TH ALIGN=left>Date:</TH>
<TD ALIGN=left>January 21, 2006 (15:12)</TD>
</TR>
<!--
<TR>
<TH ALIGN=left>News Source:</TH>
<TD ALIGN=left></TD>
</TR>
-->
<TR>
<TH ALIGN=left>Comment:</TH>
<TD ALIGN=left>Here is an interesting article that helps to show the controversy that can be created by statistics.  Azerbaijan is going to commission a presumably expensive study of their own to try to disprove UNICEF's study of important measures in the country, like life expectancy and other social statistics.  Azerbaijan is presumably doing this in order to find more favorable numbers than what UNICEF discovered, perhaps in the hope that this will give them better standing in the international community.  So much weight is attached to statistics in today's society, and since statistics only estimate true values, there are always bound to be people that are unhappy with them.

http://www.today.az/news/society/22336.html</TD>
</TR>
</TABLE>
<P>

<HR noshade>

<TABLE cellspacing="2" cellpadding = 5 border="0" 
BGCOLOR = "#c9dce1" WIDTH = 550>
<TR >
<TH WIDTH=75 ALIGN=left>Name:</TH>
<TD WIDTH=475 ALIGN=left>Chetan Mehta</TD>
</TR>
<TR>
<TH ALIGN=left>Date:</TH>
<TD ALIGN=left>January 18, 2006 (18:21)</TD>
</TR>
<!--
<TR>
<TH ALIGN=left>News Source:</TH>
<TD ALIGN=left></TD>
</TR>
-->
<TR>
<TH ALIGN=left>Comment:</TH>
<TD ALIGN=left>The scientists have spoken! Soccer > Baseball > Football.

http://news.yahoo.com/s/nm/20060104/sp_nm/soccer_surprise_dc

The article doesn't have enough for any statistical analysis, but I thought it was an interesting application of the field.

Just counting the number of upsets seems like an awfully narrow proxy for 'excitement'. Perhaps team-rivalry or 'average point differences during course of game' or a combination of all three would be a better approximation?</TD>
</TR>
</TABLE>
<P>

<HR noshade>

<TABLE cellspacing="2" cellpadding = 5 border="0" 
BGCOLOR = "#c9dce1" WIDTH = 550>
<TR >
<TH WIDTH=75 ALIGN=left>Name:</TH>
<TD WIDTH=475 ALIGN=left>Kasia</TD>
</TR>
<TR>
<TH ALIGN=left>Date:</TH>
<TD ALIGN=left>January 18, 2006 (17:11)</TD>
</TR>
<!--
<TR>
<TH ALIGN=left>News Source:</TH>
<TD ALIGN=left></TD>
</TR>
-->
<TR>
<TH ALIGN=left>Comment:</TH>
<TD ALIGN=left>This is a bit late, but I've been having trouble finding anything that I thought related to the topics of the class.
http://www.earthtimes.org/articles/show/5043.html
An article about positive effects of aspirin on the health of women. So far it has been shown that aspirin can help reduce the risk of heart disease in men, however, it was not shown to have any effects on women.  This study showed that using aspirin by women can reduce risk of stroke by 12%. However, doctors are cautious about prescribing aspirin therapies to people because of possible internal bleeding that sometimes happens.   It is interesting to see how benefits of a drug could make it that the drug is not nessecarity the best solution because of other possible effects.  It would be intersting to see how the two different probabilities of events interact and at what point it might be safe to take asprin and at what point it wouldnt.</TD>
</TR>
</TABLE>
<P>

<HR noshade>

<TABLE cellspacing="2" cellpadding = 5 border="0" 
BGCOLOR = "#c9dce1" WIDTH = 550>
<TR >
<TH WIDTH=75 ALIGN=left>Name:</TH>
<TD WIDTH=475 ALIGN=left>Ralph Callaway</TD>
</TR>
<TR>
<TH ALIGN=left>Date:</TH>
<TD ALIGN=left>January 18, 2006 (12:14)</TD>
</TR>
<!--
<TR>
<TH ALIGN=left>News Source:</TH>
<TD ALIGN=left></TD>
</TR>
-->
<TR>
<TH ALIGN=left>Comment:</TH>
<TD ALIGN=left><a href="http://www.dailynews.com/santaclarita/ci_3411523">Home prices jump 18% in '05</a>

The article looks at home prices in Santa Clarita which continue an unabated rise with the median house price now reaching $600k.

The record high prices and increasing interest rates are expected to slow home sales, though experts believe prices will continue to rise 5-10% in the coming years, slightly ahead of inflation.</TD>
</TR>
</TABLE>
<P>

<HR noshade>

<TABLE cellspacing="2" cellpadding = 5 border="0" 
BGCOLOR = "#c9dce1" WIDTH = 550>
<TR >
<TH WIDTH=75 ALIGN=left>Name:</TH>
<TD WIDTH=475 ALIGN=left>Mark Christman</TD>
</TR>
<TR>
<TH ALIGN=left>Date:</TH>
<TD ALIGN=left>January 18, 2006 (06:02)</TD>
</TR>
<!--
<TR>
<TH ALIGN=left>News Source:</TH>
<TD ALIGN=left></TD>
</TR>
-->
<TR>
<TH ALIGN=left>Comment:</TH>
<TD ALIGN=left>http://www.rand.org/pubs/technical_reports/2005/RAND_TR292.pdf

This is a report published by the RAND Corporation which studies the variablity of energy price-elasticity across regions, states, and sub-regions.  The article examines how demand varies with fluctuations in prices in order to better model the demand curve energy companies face.  This analysis of variance allows market analysts to determine the extent to which energy companies may be exploiting market power (taking advantage of areas in which demand is inelastic).  The point of interest in the article is that it reports summary statistics that seem suggestive of discrepancies in price elasticity across regions, but the variance of the data is too high for the differences to be statistically significant.</TD>
</TR>
</TABLE>
<P>

<HR noshade>

<TABLE cellspacing="2" cellpadding = 5 border="0" 
BGCOLOR = "#c9dce1" WIDTH = 550>
<TR >
<TH WIDTH=75 ALIGN=left>Name:</TH>
<TD WIDTH=475 ALIGN=left>Seth Zimmerman</TD>
</TR>
<TR>
<TH ALIGN=left>Date:</TH>
<TD ALIGN=left>January 17, 2006 (10:48)</TD>
</TR>
<!--
<TR>
<TH ALIGN=left>News Source:</TH>
<TD ALIGN=left></TD>
</TR>
-->
<TR>
<TH ALIGN=left>Comment:</TH>
<TD ALIGN=left>http://www.iht.com/articles/2006/01/16/news/flu.php

This article discusses a U.N. recommendation that European countries should systematically check incoming airline travelers for goods that might contribute to the spread of bird flu (e.g., chicken parts). Apparently, officials have been confining themselves to spot-checks of baggage, and, in one sample of 300 flights, they found 9.5 tons of material that should not have been permitted aboard. 

This raises a few interesting statistical and behavioral questions. First, what is the chance that the results of the one cited search are "typical?" I imagine this would depend on the distribution of the weight  between the passengers; i.e., if one guy had all 9.5 tons of chicken, it's quite possible the result was a fluke, but if every passenger brought their own chicken, the result would be very conclusive. Additionally, there's the question of how random sampling affects people's behavior; that is, what level of random sampling would it take for the perceived risk of confiscation to be great enough to discourage people from bringing  banned items? Depending on the answer to these two questions, is it possible there might be a cheaper, more effective way to deter people from bringing unwanted baggage than simply asking everyone to list their possessions? </TD>
</TR>
</TABLE>
<P>

<HR noshade>

<TABLE cellspacing="2" cellpadding = 5 border="0" 
BGCOLOR = "#c9dce1" WIDTH = 550>
<TR >
<TH WIDTH=75 ALIGN=left>Name:</TH>
<TD WIDTH=475 ALIGN=left>Chetan Mehta</TD>
</TR>
<TR>
<TH ALIGN=left>Date:</TH>
<TD ALIGN=left>January 17, 2006 (10:23)</TD>
</TR>
<!--
<TR>
<TH ALIGN=left>News Source:</TH>
<TD ALIGN=left></TD>
</TR>
-->
<TR>
<TH ALIGN=left>Comment:</TH>
<TD ALIGN=left>http://www.w3schools.com/browsers/browsers_stats.asp

Here's an interesting site that tracks browser statistics. It's a  site for web professionals, so Firefox is overrepresented, a bias admitted by the webmasters. But it can serve as a general proxy for the rest of the Web. More interestingly though, Firefox makes gains every month, but not consistently. They're in the same range, but differ by 1-2 percentage points. Sometimes it drops. I thought it would be interesting to find out a couple of things to find the overall usage of the browsers:

a) The standard deviation of the percentages, to see if the downturns in monthly gains for Firefox are in fact significant.

b) How does this chart the overall growth in the internet population? Is Firefox taking users away from IE or is it hitching on to new internet users?

c) If we had official web statistics, it would be interesting to see how close of a correlation there is between the percentages in the two markets.</TD>
</TR>
</TABLE>
<P>

<HR noshade>

<TABLE cellspacing="2" cellpadding = 5 border="0" 
BGCOLOR = "#c9dce1" WIDTH = 550>
<TR >
<TH WIDTH=75 ALIGN=left>Name:</TH>
<TD WIDTH=475 ALIGN=left>Dave Raines</TD>
</TR>
<TR>
<TH ALIGN=left>Date:</TH>
<TD ALIGN=left>January 16, 2006 (22:07)</TD>
</TR>
<!--
<TR>
<TH ALIGN=left>News Source:</TH>
<TD ALIGN=left></TD>
</TR>
-->
<TR>
<TH ALIGN=left>Comment:</TH>
<TD ALIGN=left>http://archives.cnn.com/2001/WORLD/europe/07/23/eu.cars/index.html

An article about price differences in cars throughout europe. Consumer groups alledge that car manufacturers are charging different prices for the same cars in different markets within the euro zone. Since the car requirements and standards are identical throughout the zone, the cars are identical and should cost the same. There is some evidence that cars are more expensive in the richer markets (specifically germany), but the manufacturers claim that statistical analysis shows the standard deviation for car prices between markets is 9% throughout europe, which is on par to the deviation for other goods. statistics to the rescue again.</TD>
</TR>
</TABLE>
<P>

<HR noshade>

<TABLE cellspacing="2" cellpadding = 5 border="0" 
BGCOLOR = "#c9dce1" WIDTH = 550>
<TR >
<TH WIDTH=75 ALIGN=left>Name:</TH>
<TD WIDTH=475 ALIGN=left>Alison</TD>
</TR>
<TR>
<TH ALIGN=left>Date:</TH>
<TD ALIGN=left>January 16, 2006 (21:25)</TD>
</TR>
<!--
<TR>
<TH ALIGN=left>News Source:</TH>
<TD ALIGN=left></TD>
</TR>
-->
<TR>
<TH ALIGN=left>Comment:</TH>
<TD ALIGN=left>Cool astrophysics stuff with standard deviation: We now know that most galaxies have super-massive black holes in their centers. Astrophysicists can even measure the masses of these black holes for nearby galaxies. And it turns out that the mass of the black hole correlates with the velocity dispersion of the stars in the galaxy that are orbiting around the black hole. This 'velocity dispersion' is just the now familiar standard deviation of the velocities of the stars. The basic idea is that  the more chaotically the stars are moving near the center of a galaxy (the higher the standard deviation of their velocities) the more massive the galaxy's black hole is.  
This is kind of a technical article, but the abstract is fairly intelligible: http://www.journals.uchicago.edu/ApJ/journal/issues/ApJ/v574n2/55401/55401.html</TD>
</TR>
</TABLE>
<P>

<HR noshade>

<TABLE cellspacing="2" cellpadding = 5 border="0" 
BGCOLOR = "#c9dce1" WIDTH = 550>
<TR >
<TH WIDTH=75 ALIGN=left>Name:</TH>
<TD WIDTH=475 ALIGN=left>Pauline</TD>
</TR>
<TR>
<TH ALIGN=left>Date:</TH>
<TD ALIGN=left>January 16, 2006 (17:11)</TD>
</TR>
<!--
<TR>
<TH ALIGN=left>News Source:</TH>
<TD ALIGN=left></TD>
</TR>
-->
<TR>
<TH ALIGN=left>Comment:</TH>
<TD ALIGN=left>Recently, the U.S. government has begun to investigate the use of biometrics, defined as the technology or scientific processes used to identify the physical characteristics of a person, with the border control. Some biotechnology officials believe that securing the borders only with biometrics may never be possible because of the two types of errors that can occur, false positives and false negatives. In this situation, a false negative is when a person is denied access to a system through a fingerprint scanner when the person is actually allowed access. A false positive occurs when the system allows access to an impostor. The algorithms behind the system must be adapted to operate in the real world by allowing for inconsistency. For instance, sweaty fingers may cause a false negative. Because of all these sensitivities, it will be quite sometime before the government uses biometrics exclusively to protect highly sensitive and top secret areas.

http://msnbc.msn.com/id/10873957/</TD>
</TR>
</TABLE>
<P>

<HR noshade>

<TABLE cellspacing="2" cellpadding = 5 border="0" 
BGCOLOR = "#c9dce1" WIDTH = 550>
<TR >
<TH WIDTH=75 ALIGN=left>Name:</TH>
<TD WIDTH=475 ALIGN=left>Tom</TD>
</TR>
<TR>
<TH ALIGN=left>Date:</TH>
<TD ALIGN=left>January 16, 2006 (15:19)</TD>
</TR>
<!--
<TR>
<TH ALIGN=left>News Source:</TH>
<TD ALIGN=left></TD>
</TR>
-->
<TR>
<TH ALIGN=left>Comment:</TH>
<TD ALIGN=left>This past weekend, a study came out from the Center for Disease control showing that two drugs commonnly used to treat influenza are quickly becoming ineffective, and that the resistance rates of influenza to both drugs is approaching 91%. This is up from a 1% resistance rate last year. It is interesting to note this startling change occurred in just a one year time period, and this could have startling effects considering a fairly high worldwide incidence rate (12%)

http://www.nytimes.com/2006/01/15/health/15drugs.html</TD>
</TR>
</TABLE>
<P>

<HR noshade>

<TABLE cellspacing="2" cellpadding = 5 border="0" 
BGCOLOR = "#c9dce1" WIDTH = 550>
<TR >
<TH WIDTH=75 ALIGN=left>Name:</TH>
<TD WIDTH=475 ALIGN=left>Neha</TD>
</TR>
<TR>
<TH ALIGN=left>Date:</TH>
<TD ALIGN=left>January 16, 2006 (13:55)</TD>
</TR>
<!--
<TR>
<TH ALIGN=left>News Source:</TH>
<TD ALIGN=left></TD>
</TR>
-->
<TR>
<TH ALIGN=left>Comment:</TH>
<TD ALIGN=left>Here is the adress for the article:
http://www.nytimes.com/2006/01/16/science/16gene.html</TD>
</TR>
</TABLE>
<P>

<HR noshade>

<TABLE cellspacing="2" cellpadding = 5 border="0" 
BGCOLOR = "#c9dce1" WIDTH = 550>
<TR >
<TH WIDTH=75 ALIGN=left>Name:</TH>
<TD WIDTH=475 ALIGN=left>Neha</TD>
</TR>
<TR>
<TH ALIGN=left>Date:</TH>
<TD ALIGN=left>January 16, 2006 (13:54)</TD>
</TR>
<!--
<TR>
<TH ALIGN=left>News Source:</TH>
<TD ALIGN=left></TD>
</TR>
-->
<TR>
<TH ALIGN=left>Comment:</TH>
<TD ALIGN=left>A NY Times article states that a certain gene, TCF7L2, carried by 1/3 of the US population results in an increased risk of Type 2 Diabetes.  Type 2 diabetes is predominant form of diabetes, accounting for 95% of all cases.  A patients risk depends on whether they have inherited the gene from one or both of their parents.  38% of Americans are estimated to have inherited 1 copy of the gene and face a 45% greater risk then people who do not have the gene.  An estimated 7% of the American population has two copies of the gene and they face a 141% increased risk then people who do not have the gene.  Scientist also state that if the gene were completely removed from the population, it would also remove 21% of all diabetes.</TD>
</TR>
</TABLE>
<P>

<HR noshade>

<TABLE cellspacing="2" cellpadding = 5 border="0" 
BGCOLOR = "#c9dce1" WIDTH = 550>
<TR >
<TH WIDTH=75 ALIGN=left>Name:</TH>
<TD WIDTH=475 ALIGN=left>SK</TD>
</TR>
<TR>
<TH ALIGN=left>Date:</TH>
<TD ALIGN=left>January 15, 2006 (12:58)</TD>
</TR>
<!--
<TR>
<TH ALIGN=left>News Source:</TH>
<TD ALIGN=left></TD>
</TR>
-->
<TR>
<TH ALIGN=left>Comment:</TH>
<TD ALIGN=left>Here is an article that originally came from the Wall Street Journal:
http://www.bradenton.com/mld/bradenton/13623116.htm?template=contentModules/printstory.jsp

The article concerns a study of a TV game show called "Deal or No Deal", which, unlike other video games, does not need skill (in this game, there are 26 briefcases of money, and the amounts--different in each container and ranging from 1 cent to $1 million--in each batch are unknown to the contestant. The player picks one case as his/her own, and then opens 24 cases. At the end, the player decides whether or not to trade his/her case with the last unopened one). It concerns a study conducted by scientists to understand what determines the choices people make in this game, which they feel would also have implication on financial behavior for allocating money into investment portfolios and government safety nets. They want to know what contestants are likely to do in this game and in other investment sitautions. Namely, would players follow the probability model, which is based on expected value? Or would they follow the model proposed by classical economics, which concerns the player's net value (in this case, most players would take a certain payoff despite favorable odds)? Or would they follow yet another model, one proposed by behavioral economists, who adhere to prospect theory, where players evaluate prospects for gains and losses from various points that could influence psychological decisions and which may shift over time? What really influences people's decision making, and are probability and expected values among these factors?</TD>
</TR>
</TABLE>
<P>

<HR noshade>

<TABLE cellspacing="2" cellpadding = 5 border="0" 
BGCOLOR = "#c9dce1" WIDTH = 550>
<TR >
<TH WIDTH=75 ALIGN=left>Name:</TH>
<TD WIDTH=475 ALIGN=left>Alison Crocker</TD>
</TR>
<TR>
<TH ALIGN=left>Date:</TH>
<TD ALIGN=left>January 13, 2006 (11:50)</TD>
</TR>
<!--
<TR>
<TH ALIGN=left>News Source:</TH>
<TD ALIGN=left></TD>
</TR>
-->
<TR>
<TH ALIGN=left>Comment:</TH>
<TD ALIGN=left>I tried to make "300 Millionth American" a link, but I guess failed, because it's not blue, here's the address: http://www.nytimes.com/2006/01/13/national/13baby.html?hp&ex=1137214800&en=965005cbf6337137&ei=5094&partner=homepage</TD>
</TR>
</TABLE>
<P>

<HR noshade>

<TABLE cellspacing="2" cellpadding = 5 border="0" 
BGCOLOR = "#c9dce1" WIDTH = 550>
<TR >
<TH WIDTH=75 ALIGN=left>Name:</TH>
<TD WIDTH=475 ALIGN=left>Alison Crocker</TD>
</TR>
<TR>
<TH ALIGN=left>Date:</TH>
<TD ALIGN=left>January 13, 2006 (11:48)</TD>
</TR>
<!--
<TR>
<TH ALIGN=left>News Source:</TH>
<TD ALIGN=left></TD>
</TR>
-->
<TR>
<TH ALIGN=left>Comment:</TH>
<TD ALIGN=left><a ref="http://www.nytimes.com/2006/01/13/national/13baby.html?hp&ex=1137214800&en=965005cbf6337137&ei=5094&partner=homepage"> 300 Millionth American</a>

    This article from the New York Times, based on info from the Census Bureau is filled with statistics stuff- birth rate is currently one birth every 8 seconds, one death every 12 seconds and one immigration every 31 seconds. While these rates are all variable (the article notes that the birth rate peaks in the summer), the Census Bureau is now estimating the arrival sometime in October.
    The article does a good job of emphasizing that a more specific guess to the date would be premature- "Projections are subject to unimaginable imponderables - from the impact of wars and epidemics to dramatic gains in life expectancy." However, the article is filled with different people's guesses on who the 300 millionth American will be. For example, it quotes Dr. William Frey with the University of Michigan Population Studies Center as saying, "The 300 millionth will be a Mexican Latino in Los Angeles County, with parents who speak Spanish at home and with siblings who are bilingual." While this might be a fairly likely outcome, it can't be all that likely- look at all the requirements! That the 300 millionth American be a newborn AND Mexican Latino AND born in Los Angeles County AND speak Spanish at home. The probability of the 300 millionth American meeting all these criteria must be pretty slim. 
    With a sample size of 1, it is really hard to say who might be the 300th American and such guess as presented in the article seem a bit silly. But, in another way, the guesses indicate who is perhaps the most typical new American at this point in history.
</TD>
</TR>
</TABLE>
<P>

<HR noshade>

<TABLE cellspacing="2" cellpadding = 5 border="0" 
BGCOLOR = "#c9dce1" WIDTH = 550>
<TR >
<TH WIDTH=75 ALIGN=left>Name:</TH>
<TD WIDTH=475 ALIGN=left>Ralph Callaway</TD>
</TR>
<TR>
<TH ALIGN=left>Date:</TH>
<TD ALIGN=left>January 09, 2006 (10:40)</TD>
</TR>
<!--
<TR>
<TH ALIGN=left>News Source:</TH>
<TD ALIGN=left></TD>
</TR>
-->
<TR>
<TH ALIGN=left>Comment:</TH>
<TD ALIGN=left>http://www.sumeria.net/aids/false.html

The above link is to early article (1988) discussing scrutinizing the accuracy of early HIV tests, immediately before the FDA attempted to ban home testing in 1989.  What is interesting about the article is that even though its expressed subject is the "thousands of HIV false positives in the US" the majority of the article discusses only the false positive rate when the subject is infected.  However, thankfully it makes one mention of when the two current test were combined to obtain a sensitivity of 99.89%, testing a population of 10,000 (assuming 2 infections/10k) would yield 10 false positives, and 2 hits (true positives).  Its shocking that even with only a .11% failure rate, 83% of positive results would be false.</TD>
</TR>
</TABLE>
<P>

<HR noshade>

<TABLE cellspacing="2" cellpadding = 5 border="0" 
BGCOLOR = "#c9dce1" WIDTH = 550>
<TR >
<TH WIDTH=75 ALIGN=left>Name:</TH>
<TD WIDTH=475 ALIGN=left>Mark Christman</TD>
</TR>
<TR>
<TH ALIGN=left>Date:</TH>
<TD ALIGN=left>January 09, 2006 (10:12)</TD>
</TR>
<!--
<TR>
<TH ALIGN=left>News Source:</TH>
<TD ALIGN=left></TD>
</TR>
-->
<TR>
<TH ALIGN=left>Comment:</TH>
<TD ALIGN=left>This comment is inspired by all the research I did when I was deciding where I wanted to work after college.  Whenever looked up figures about "cost of living" in different cities in the united states, the ACCRA quarterly report on cost of living in different regions was cited as the primary source.  I decided to analyze this report to see exactly what went into calculating the numbers.

What I found is that the numbers are completely bogus for most practical applications.  An example is that ACCRA uses the median rent statistics in calculating housing costs (more specifically, the median price based on magnitude, not frequency).  When looking in a city like DC, its median rent price was similar to Boston.  However, the modal price was significantly cheaper in DC.  

Since housing is a major part of cost of living, it made DC look much more expensive to live in than it actually was.  This is just an example that shows statistics can often be misused, and thus misrepresent reality.</TD>
</TR>
</TABLE>
<P>

<HR noshade>

<TABLE cellspacing="2" cellpadding = 5 border="0" 
BGCOLOR = "#c9dce1" WIDTH = 550>
<TR >
<TH WIDTH=75 ALIGN=left>Name:</TH>
<TD WIDTH=475 ALIGN=left>Dave Raines</TD>
</TR>
<TR>
<TH ALIGN=left>Date:</TH>
<TD ALIGN=left>January 08, 2006 (21:15)</TD>
</TR>
<!--
<TR>
<TH ALIGN=left>News Source:</TH>
<TD ALIGN=left></TD>
</TR>
-->
<TR>
<TH ALIGN=left>Comment:</TH>
<TD ALIGN=left>http://money.cnn.com/2005/12/06/technology/opinion_fsb/index.htm

An interesting article about a firm which offers online polling for market research to small businesses. It discusses the issues of online polling, as well as the problems of polling in general: low response rates combined with a bias in the type of people who do respond (both online and on the phone), also the difficulty of posing useful questions, which is especially difficult when the question is only in text form.
The article shows that all statistics coming from people need to be examined carefully to see what kind of bias there was in the phrasing of the questions, or in the group being sampled.</TD>
</TR>
</TABLE>
<P>

<HR noshade>

<TABLE cellspacing="2" cellpadding = 5 border="0" 
BGCOLOR = "#c9dce1" WIDTH = 550>
<TR >
<TH WIDTH=75 ALIGN=left>Name:</TH>
<TD WIDTH=475 ALIGN=left>Neha</TD>
</TR>
<TR>
<TH ALIGN=left>Date:</TH>
<TD ALIGN=left>January 08, 2006 (20:17)</TD>
</TR>
<!--
<TR>
<TH ALIGN=left>News Source:</TH>
<TD ALIGN=left></TD>
</TR>
-->
<TR>
<TH ALIGN=left>Comment:</TH>
<TD ALIGN=left>A recent ABC news article discussed the probability of an eyewitness correctly identifying a criminal.  The article created an experiment with 3 pennies where the "culprit" penny lands on heads 75% ofthe time.  After three consecutive heads, Bayes theorem states that the probability of picking the correct penny is 63%.  The article states that this penny example is analogous to "what we do when we change our estimate of the probability of a suspect's guilt after the testimony of an eyewitness".  Professor Wells, Iowa State University, states that only 60% of the time a correct identification is made and innocents in a line up are picked as high 20% of the time.  Professor Wells went on to say that the initial probability of a suspect being a culprit greatly impacts the final result.</TD>
</TR>
</TABLE>
<P>

<HR noshade>

<TABLE cellspacing="2" cellpadding = 5 border="0" 
BGCOLOR = "#c9dce1" WIDTH = 550>
<TR >
<TH WIDTH=75 ALIGN=left>Name:</TH>
<TD WIDTH=475 ALIGN=left>Kasia</TD>
</TR>
<TR>
<TH ALIGN=left>Date:</TH>
<TD ALIGN=left>January 08, 2006 (20:13)</TD>
</TR>
<!--
<TR>
<TH ALIGN=left>News Source:</TH>
<TD ALIGN=left></TD>
</TR>
-->
<TR>
<TH ALIGN=left>Comment:</TH>
<TD ALIGN=left>OraQuick Advance, an oral test that uses saliva to test for HIV within 20 minutes, has been creating more false positive results than desired.  The CDC data from early 2005 showed that the test was 99.8% accurate for people with the virus.  However, more recent results have shown that it is as low as 99.1% accurate: a difference that seems rather small but when a large group of people get tested, the number of false positive can be significant.   Although most tests are expected to produce some false positive results, and therefore require additional testing, it is interesting to see how small a difference in accuracy can results an increased number of false positive results.  Also, according to the article, many testing centers are stopping to use OraQuick Advance because of its false positive results rate.  However, it does not mention whether it also produces false negative results.  Woudn’t it be better to develop a test that is 100% (or close) accurate for negative results, but still produces some false positives, than a test that produces less false positives but more false negatives?  Source: http://www.contracostatimes.com/mld/cctimes/news/state/13485098.htm</TD>
</TR>
</TABLE>
<P>

<HR noshade>

<TABLE cellspacing="2" cellpadding = 5 border="0" 
BGCOLOR = "#c9dce1" WIDTH = 550>
<TR >
<TH WIDTH=75 ALIGN=left>Name:</TH>
<TD WIDTH=475 ALIGN=left>SK</TD>
</TR>
<TR>
<TH ALIGN=left>Date:</TH>
<TD ALIGN=left>January 08, 2006 (20:03)</TD>
</TR>
<!--
<TR>
<TH ALIGN=left>News Source:</TH>
<TD ALIGN=left></TD>
</TR>
-->
<TR>
<TH ALIGN=left>Comment:</TH>
<TD ALIGN=left>I read an article with these links:

http://chance.dartmouth.edu/chancewiki/index.php/Chance_News_11#Mothers_Know_Best.__Or_Do_They.3F
http://homepages.utoledo.edu/pfritz/_news/news-2428.htm

(need to scroll down for the second link; it's a Wall Street Journal article from 12/27/05 entitled: "At Medical Journal, Editor Finds Truth Hard to Track Down")

The article centers on an editor of the British Medical Journal who essentially spent 12 years trying to scrutinize a much-cited study performed in India that claimed that fiber-rich foods could dramatically lower the chance of death by heart attack (it was no surprise that there would be a difference, just that no one would expect that it would be that large). The editor, Dr. Richard Smith, became concerned about the article when two letters received two letters questioning the validity of the study (they both asked how the person conducting the research, Dr. Ram Singh, could have mmanaged five trials--three of which used over 400 subjects--within 18 months without using the same subjects in momre than one study or performing simultaneous tests). Eventually, Dr. Smith called on a statistician to analyze the article and the study. The statistician said that he needed to look at the full data to examine everything, which was not possible because, according to Dr. Singh, termites ate up the data and other records. The statistician, however, used data from some of other Dr. Singh's articles that were also submitted to the BMJ, and he found that the data "were either fabricated or falsified" since there were many statistical errors and contradictions. In addition, the patterns of the subjects' ages differed between the two groups, which suggests shouldn't have occurred if the subjects were randomly selected. Dr. Singh, however, contends that the irregulaties came from frequent power outages and lack of funding. Another editor contended that the fact that the research came from a place with differing cultures and institutions not parallel to those in the US or Europe has also complicated matters. With all of these things in mind, one has consider: i) what is the probability that data is fake, especially given that the sample data looks irregular? ii) do different research situations (ie. culture, technology, and institutions) increase the chance that there will be some kind of misunderstanding? iii) could this sort of investigation be quantified into probabilities, like a numeric score? iv) should there be an acceptable probability for determining if something is not a fraud?</TD>
</TR>
</TABLE>
<P>

<HR noshade>

<TABLE cellspacing="2" cellpadding = 5 border="0" 
BGCOLOR = "#c9dce1" WIDTH = 550>
<TR >
<TH WIDTH=75 ALIGN=left>Name:</TH>
<TD WIDTH=475 ALIGN=left>Pauline</TD>
</TR>
<TR>
<TH ALIGN=left>Date:</TH>
<TD ALIGN=left>January 08, 2006 (19:17)</TD>
</TR>
<!--
<TR>
<TH ALIGN=left>News Source:</TH>
<TD ALIGN=left></TD>
</TR>
-->
<TR>
<TH ALIGN=left>Comment:</TH>
<TD ALIGN=left>Paul Newton of the University of Southern California and Joseph Keller of Stanford University have developed formulas predicting a tennis player's chances of winning. Besides looking at the rankings of the players who are matched up against each other, an important fact to discover is the probability that each player wins a rally against the other player when serving. Based on their model, Newton and Keller prove that the probability of winning either a set or the match does not actually depend on which player serves first. When comparing their formulas against the data from the 2002 U.S. Open singles matches, the results are quite similar. The development of these formulas could add another interesting element to sport-watchers when trying to place their bets on who will win the match. Somewhat disappointingly, the method of how Newton and Keller developed these formulas was not discussed in the article.
http://www.sciencenews.org/articles/20050611/mathtrek.asp</TD>
</TR>
</TABLE>
<P>

<HR noshade>

<TABLE cellspacing="2" cellpadding = 5 border="0" 
BGCOLOR = "#c9dce1" WIDTH = 550>
<TR >
<TH WIDTH=75 ALIGN=left>Name:</TH>
<TD WIDTH=475 ALIGN=left>Tom</TD>
</TR>
<TR>
<TH ALIGN=left>Date:</TH>
<TD ALIGN=left>January 08, 2006 (18:57)</TD>
</TR>
<!--
<TR>
<TH ALIGN=left>News Source:</TH>
<TD ALIGN=left></TD>
</TR>
-->
<TR>
<TH ALIGN=left>Comment:</TH>
<TD ALIGN=left>This past year, the issue of steroid and drug abuse in major league sports has received tremendous news coverage. The Comissioner of Major League Baseball, Bud Selig, enacted a new policy in the minor leagues that issues incredibly strong punishments for steroid violations: first offence, 50 games; second offense, 100 games; third offense, lifetime ban. However warranted these punishments may be for such illegal activity as steroid abuse (especially occurring in the role models of many children), the issue of false positives in these tests should also be considered. Even a single false-positive could result in a professional player missing 50 games of his season, costing him several million dollars in salary and many more in image and sponsorships. A great example of this is Kobe Bryant; although he was found not guilty in his trial, he still lost almost all of his sponsorship deals due to the speculation surrounding his guilt. There is an estimated steroid abuse rate of 1.7% in major league baseball (http://strikethree.com/2005/03/18/a-tale-of-two-hearings/), and even with a false-positive rate of 1 in 10,000 (no exact statistics could be found, this is an estimate of several statistics I found), there could be tremendous consequences for the several thousand players in major and minor league baseball.

Information also taken from : http://www.azcentral.com/arizonarepublic/sports/articles/0518steroids0518.html
</TD>
</TR>
</TABLE>
<P>

<HR noshade>

<TABLE cellspacing="2" cellpadding = 5 border="0" 
BGCOLOR = "#c9dce1" WIDTH = 550>
<TR >
<TH WIDTH=75 ALIGN=left>Name:</TH>
<TD WIDTH=475 ALIGN=left>Brian</TD>
</TR>
<TR>
<TH ALIGN=left>Date:</TH>
<TD ALIGN=left>January 08, 2006 (14:04)</TD>
</TR>
<!--
<TR>
<TH ALIGN=left>News Source:</TH>
<TD ALIGN=left></TD>
</TR>
-->
<TR>
<TH ALIGN=left>Comment:</TH>
<TD ALIGN=left>This is a timely bit of statistics for anyone who has enjoyed following the college bowl games and now the NFL playoffs.  Much has been made in the past several years over the ways in which the different football leagues conduct overtimes.  The NFL uses a sudden-death format in which a coin toss determines which team starts with the ball.  Obviously, it is to a team's advantage to win the coin toss; the team that does so scores on their initial possesion about 1/4 of the time and wins the game 58% of the time historically.

The NCAA uses a different and supposedly more equitable format.  In each overtime, each team gets a chance to start with the ball 25 yards from the end zone.  Even if the team that starts with the ball first scores, the other team still gets the ball with a chance to equal or better their opponent's result.  When the NCAA switched to this format sein 1996, it was lauded as a fair way to conduct overtime as each team was guaranteed at least one possession.

However, it may be that people's perception of this system as "fair" is actually flawed.  As of 2001, the team winning the coin toss ended up winning 63% of the time - suggesting that the coin toss carried more weight than in the supposedly unfair NFL.  Why is winning the coin toss an advantage?  Because by doing so, that team can choose to receive the ball second in the overtime, and they will already know how many points their opponent has scored.  On the contrary, the team that loses the toss and gets the ball first doesn't know how many points they need to score in order to be "safe."  For instance, on 4th and 2, should they try for the first down and keep pressing toward the end zone, or kick the field goal and take 3 points, knowing full well that this result may not be sufficient to keep their opponent from winning the game?

This dilemma presents a couple of interesting questions.  To what extent does our perception of fairness influence our policies, and why aren't statistics that go against our perceptions given more publicity?  Major sports networks like ESPN never talk about the bias in college football overtime.  Does it make for better TV as long as people think it's fair, and so there's no reason to tell them otherwise?

Links: http://www.collegeplayoffs.com/2000_tour.htm
http://www.sciencenews.org/articles/20041106/mathtrek.asp
</TD>
</TR>
</TABLE>
<P>

<HR noshade>

<TABLE cellspacing="2" cellpadding = 5 border="0" 
BGCOLOR = "#c9dce1" WIDTH = 550>
<TR >
<TH WIDTH=75 ALIGN=left>Name:</TH>
<TD WIDTH=475 ALIGN=left>Seth</TD>
</TR>
<TR>
<TH ALIGN=left>Date:</TH>
<TD ALIGN=left>January 08, 2006 (13:16)</TD>
</TR>
<!--
<TR>
<TH ALIGN=left>News Source:</TH>
<TD ALIGN=left></TD>
</TR>
-->
<TR>
<TH ALIGN=left>Comment:</TH>
<TD ALIGN=left>From the Washington Post: "two winters ago, what had been a mediocre safety record at West Virginia's Sago Mine grew dramatically worse. Over 23 months beginning in February 2004, two dozen miners were hurt in a string of accidents, some of them caused by rock chunks falling from the mine ceiling. Federal safety inspectors slapped the mine with citations 273 times, or an average of once every 2 1/2 days.

Despite this record, the price paid by Sago's operators was light. Government regulators never publicly discussed shutting down the mine and never sought criminal sanctions. The biggest single fine was $440, about 0.0004 percent of the $110 million net profit reported last year by the mine's current owner, International Coal Group Inc."

The article raises questions like a) at what point does a random fluctuation in mine safety become something we should pay more attention to? and b) what should be done with statistics on mine safety, anyway? Is there an acceptable level of mining accidents/fatalities? Can you statistically separate negligence from the unavoidable dangers of the profession? 

http://www.washingtonpost.com/wp-dyn/content/article/2006/01/07/AR2006010700967.html</TD>
</TR>
</TABLE>
<P>

<HR noshade>

<TABLE cellspacing="2" cellpadding = 5 border="0" 
BGCOLOR = "#c9dce1" WIDTH = 550>
<TR >
<TH WIDTH=75 ALIGN=left>Name:</TH>
<TD WIDTH=475 ALIGN=left>Alex</TD>
</TR>
<TR>
<TH ALIGN=left>Date:</TH>
<TD ALIGN=left>January 07, 2006 (10:02)</TD>
</TR>
<!--
<TR>
<TH ALIGN=left>News Source:</TH>
<TD ALIGN=left></TD>
</TR>
-->
<TR>
<TH ALIGN=left>Comment:</TH>
<TD ALIGN=left>I heard on NPR yesterday that a "Justice Department report says FBI overconfidence led it to disregard warnings that it had wrongly identified a fingerprint in the Madrid bombing case. Spanish officials tried to tell the FBI that the fingerprint associated with the case didn't match that of Oregon attorney Brandon Mayfield. The inspector general also found the FBI exhibited bias by clinging to Mayfield as a suspect because he is Muslim and had represented a terrorism suspect." (I cut that from NPR website).

What's so interesting is how once the FBI got onto the idea,
evidence became subjectively distorted or overlooked to support their beliefs. How often does this happen but we don't hear about it?

It begs some questions about the statistical procedure of matching fingerprints, such as
i) what is the false positives rate for fingerprints? Does this depend on whether the FBI subsequently learns (as they did) that the suspect is a Muslim?
ii) shouldn't all evidence be more realistically presented as a probability? 
How would the legal system have to change if a jury had to return a verdict p(guilty) = 0.7 ?

See eg <a href="http://www.chicagotribune.com/news/specials/chi-0411140299nov14,1,1550875.story?coll=chi-newsspecials-hed">
Chigaco Tribune</a> which also has interesting collection of botched forensics articles...</TD>
</TR>
</TABLE>
<P>

<HR noshade>

<TABLE cellspacing="2" cellpadding = 5 border="0" 
BGCOLOR = "#c9dce1" WIDTH = 550>
<TR >
<TH WIDTH=75 ALIGN=left>Name:</TH>
<TD WIDTH=475 ALIGN=left>alex</TD>
</TR>
<TR>
<TH ALIGN=left>Date:</TH>
<TD ALIGN=left>January 04, 2006 (11:23)</TD>
</TR>
<!--
<TR>
<TH ALIGN=left>News Source:</TH>
<TD ALIGN=left></TD>
</TR>
-->
<TR>
<TH ALIGN=left>Comment:</TH>
<TD ALIGN=left>testing the comments page</TD>
</TR>
</TABLE>
<P>

<HR noshade>


<!-- WARNING: Don't mess with the stuff above. -->


</BODY>
</HTML>


